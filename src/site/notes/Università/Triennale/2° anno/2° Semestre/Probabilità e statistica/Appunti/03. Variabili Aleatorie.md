---
{"dg-publish":true,"permalink":"/universita/triennale/2-anno/2-semestre/probabilita-e-statistica/appunti/03-variabili-aleatorie/"}
---


# 3. Variabili Aleatorie

#ArgomentoFondamentale 

```ad-Definizione
title: Variabile Aleatoria

In generale, una variabile si dice **aleatoria** se esiste una **distribuzione di probabilità**.
Infatti le probabilità possono essere **uniformi** oppure no

```

### Variabile aleatoria Degenere

```ad-Definizione
title: VA Degenere - $X \sim \mathrm{Deg}(a)$

$X \sim \mathrm{Deg}(a)$ con $a \in \mathbb{R}$ è detta variabile aleatoria degenere ed è quella tale per cui:
$
\mathcal{P}(X=a) = 1
$

```

Ne deriva che:
- $EX = a$ (la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Media\|#Media]])
- $\mathrm{Var}(X) = 0$ (la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Varianza\|#Varianza]])

![GraficoDistribuzioneDegenere 1.png|Andamento funzione di ripartizione di una variabile degenere](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoDistribuzioneDegenere%201.png)

## Media
#### Media di variabili aleatorie Discrete

```ad-Definizione
title: Media di Variabili aleatorie Discrete ($E[X]$)


$\Omega \subset \mathbb{Z}$ o numerabile
Sia $X$ la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#3. Variabili Aleatorie\|variabile aleatoria]] su $\Omega$ con probabilità $\mathcal{P}(X=x)$, $x \in \Omega$
Definisco la **media** di $X$ il valore
$
E[X] = \sum\limits_{x\in \Omega} x \cdot \mathcal{P}(X=x)
$
Ossia la media dei valori di $\Omega$ presi con il loro peso. (E sta per Expectation)

```

#### Media di variabili aleatorie Continue

```ad-Definizione
title: Media di Variabili Aleatorie Continue ($EX$)

$
EX = \int\limits_\mathbb{R} x \cdot f_{X}(x) \, dx
$

```

### Proprietà della media
#### Linearità della media

```ad-Teo
title: Linearità della media

La media, sia continua che discreta, è lineare:


$
E[aX+b] = aEX + b
$

___
**Dimostrazione**

Sia $g(x)$ una funzione lineare. In questo caso $aX+b$

Per la media di una variabile continua:

$
\begin{align}
E[aX+b] &= E g(x) = \\
&= \int_{\mathbb{R}}  (ax+b)f_{X}(x) \, dx = \\
&= a \int_{\mathbb{R}}xf_{X}(x)\, dx + b \overbrace{\int_{\mathbb{R}} f_{X}(x) \, dx}^{= 1}  = \\
&= aEX + b
\end{align}
$

Mentre la media di una variabile discreta:

$
\begin{align}
E(aX+b) &= \sum\limits_{k} (ax_{k}+b)p_{k} = \\
&= a \sum\limits_{k}x_{k}p_{k} + b \overbrace{\sum\limits_{k}p_{k}}^{=1} = \\
&= aEX + b
\end{align}
$

```

## Momento

```ad-Definizione
title: Momento di VA di rordine $r>0$

Il **momento di una VA è definito come:**
$
EX^{r} = \sum\limits_{k}(x_{k})^{r}p_{k}
\qquad
EX^{r} = \int_\mathbb{R} x^{r}f_{X}(x) \, dx
$
a seconda che $X$ sia una variabile aleatoria discreta o continua. 

si definiranno:
- $r = 1$: Primo momento (coincide con la [[#Media]])
- $r = 2$: Secondo momento

```


## Varianza

```ad-Definizione
title: Varianza $\mathrm{Var}(X)$

![GraficoVarianza 1.png|500](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoVarianza%201.png)

La **varianza** è una misura quadratica di quanto $X$ si discosta dalla sua [[#Media e momenti]] (ossia dal suo valore centrale).
$
\mathrm{Var}(X) = E(X-EX)^{2}
$
Spesso si indica la varianza con $\sigma^{2}$.

```

Facendo alcuni conti:
$$
\begin{align}
\mathrm{Var} (X) &= E(X-EX)^{2} = \\
&= E[X^{2} - 2XEX + (EX^{2})] = \\
&= EX^{2} - 2 EX \cdot \overbrace{EX}^{\mu} + \overbrace{(EX)^{2}}^{\mu^{2}} = \\
&= EX^{2} - 2(EX)^{2} + (EX)^{2} = \\
&= EX^{2} - (EX)^{2}
\end{align}
$$

Da cui vediamo che la varianza è uguale al 2° [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Momento\|#Momento]] meno il 1°:

# Variabili aleatorie DISCRETE

#### Condizioni necessarie e sufficienti per densità discreta

```ad-Teo
title: CNS per densità discreta


1. $p_{k} \ge 0$
2. $\sum\limits_{\mathbb{k} \in \mathbb{Z}} p_{k} = 1$
La funzione Discreta soddisfa le proprietà 1. e 2. **se e solo se** è una densità discreta di qualche variabile aleatoria $X$

```


## Tipi di variabili aleatorie discrete

- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Binomiale\|#Variabile Aleatoria Binomiale]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria di Bernoulli\|#Variabile Aleatoria di Bernoulli]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Geometrica\|#Variabile Aleatoria Geometrica]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria di Poisson\|#Variabile Aleatoria di Poisson]]

### Variabile Aleatoria Binomiale

```ad-Definizione
title: VA Binomiale (discreta): $X \sim \mathrm{Bin}$

Si dice che $X$ si distribuisce in accordo con una **variabile binomiale** e si scrive $X \sim \text{Bin}(n, p)$ se $X$ è di parametri $(n, p)$ con:

- $n \in \mathbb{N}$
- $p \in [0,1]$

Allora, detto $X =$ "Il numero di successi su $n$ prove indipendenti in cui il successo è un evento di probabilità $p$"
$
\begin{align}
\mathcal{P}(X=x) = \binom{n}{x}p^{x}(1-p)^{n-x} \, , && x \in \{0,1,...,n\} \in \mathbb{Z}
\end{align}
$

![GraficoVABinomiale.png](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoVABinomiale.png)
___
Ricavare la binomiale


Si supponga di avere una scatola contente una pallina con uno 0 e una pallina con un 1. Immagino di voler riempire n caselle con 0 o 1 a seconda della pallina che estraggo, ripetendo l'estrazione n volte con reimmissione
Siano
- $K$ l'evento: "Ottengo 1, k volte" (lo posso leggere anche come la somma di tutte le caselle)
- $x_{i} =$ "Valore della casella i-esima" $\in \{0,1\}$
$
K = \sum\limits_{i=1}^{n} x_{i} \qquad \in \{ 0,1,2,3,...,n\}
$
La probabilità che una qualsiasi casella abbia valore 0, o abbia valore 1, è $\frac{1}{2}$
$
\mathcal{P}(x_{i} = 0)  = \mathcal{P}(x_{i}=1) = \frac{1}{2} \quad \forall i
$
La probabilità che $K = 0$, uguale alla probabilità che K = n è:
$
\begin{align}
\mathcal{P}(K=0) = \mathcal{P}\left(\bigcap_{i=1}^{n} x_{i} = 0 \right) =\prod_{i=1}^{n} \mathcal{P}(x_{i} = 0) = \left( \frac{1}{2} \right)^{n} \\ \\
\mathcal{P}(K=n) = \mathcal{P}\left(\bigcap_{i=1}^{n} x_{i} = 1 \right) =\prod_{i=1}^{n} \mathcal{P}(x_{i} = 1) = \left( \frac{1}{2} \right)^{n}
\end{align}
$
Proviamo a calcolare la probabilità che $K = 2$:
$
\begin{align}
\mathcal{P}(K = 2) &= \mathcal{P}(  \overbrace{\text{"2 volte 1", "n-2 volte 0"}}^{ =C_{j}}) =  \\
&= \mathcal{P} \left(\bigcup_{j = 1}^{\binom{n}{2}} C_{j} \right) =  \\
&= \sum\limits_{j=1}^{\binom{n}{2}}\mathcal{P}(C_{j})
\end{align}
$
$1 \le j \le \binom{n}{2}$ perché ci sono $\binom{n}{2}$ modi di disporre 2 palline in n caselle. Si tratta infatti di una [[02. Misura e Probabilità#Combinazioni semplici|combinazione semplice]] di $n$ elementi in classe di $2$: $C_{n,2}$.

$
\mathcal{P}(C_{j}) = \left( \frac{1}{2} \right)^{2} \cdot \left( \frac{1}{2} \right)^{n-2}
$

Generalizzando otteniamo che:
$
\mathcal{P}(K = k) = \binom{n}{k} \left( \frac{1}{2} \right)^{k} \left( \frac{1}{2} \right)^{n-k}
$
Qualora l'evento considerato non abbia probabilità $\frac{1}{2}$ ma $p$ si ha:
$
\mathcal{P}(K = k) = \binom{n}{k} \left( p \right)^{k} \left( 1-p\right)^{n-k}
$


```

#### Media VA Binomiale

```ad-Teo
title: Media VA Binomiale $(EX)$

Sia $X \sim \mathrm{Bin}(n,p)$ con $n \in \mathbb{N}, \quad p \in [0,1]$
La [[#Media di variabili aleatorie Discrete]]  per la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Binomiale\|#Variabile Aleatoria Binomiale]] è:
$
EX = np
$
```

#### Varianza VA Binomiale

```ad-Teo
title: Media VA Binomiale $(\text{Var}(X))$

Sia $X \sim \mathrm{Bin}(n,p)$ con $n \in \mathbb{N}, \quad p \in [0,1]$
La [[#Varianza]]  per la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Binomiale\|#Variabile Aleatoria Binomiale]] è:
$
\mathrm{Var}(X) = np(1-p)
$
```



### Variabile Aleatoria di Bernoulli
È  un caso particolare di [[#Variabile Aleatoria Binomiale]] in cui $n = 1$

```ad-Definizione
title: VA di Bernoulli (discreta): : $X \sim \mathrm{Ber}$


$X \sim \text{Ber}(p) \equiv \text{Bin}(1, p)$
con $x \in \{0,1\}$
$
\begin{align}
\mathcal{P} (X=x) = p^{x}(1-p)^{1-x} = 
\begin{cases}
1-p & x=0 \\
p, & x=1
\end{cases}
\end{align}
$


```

#### Media VA di Bernoulli
```ad-Teo
title: Media VA di Bernoulli $(EX)$

Sia $X \sim \mathrm{Ber}(p)$ con $p \in [0,1], \quad x \in \{0,1\}$
La [[#Media di variabili aleatorie Discrete]]  per la [[#Variabile Aleatoria di Bernoulli]] è:
$
EX = p
$
```

#### Varianza VA Binomiale
```ad-Teo
title: Media VA di Brenoulli $(\text{Var}(X))$

Sia $X \sim \mathrm{Ber}(p)$ con $p \in [0,1]$
La [[#Varianza]]  per la [[#Variabile Aleatoria di Bernoulli]] è:
$
\mathrm{Var}(X) = p(1-p)
$
```


### Variabile Aleatoria Geometrica

```ad-Definizione
title: VA Geometrica (discreta)

$X \sim \text{Geo}(p)$, $p \in [0,1]$
$
\begin{align}
\mathcal{P} (X = x) = (1-p)^{x-1}p && x \in \mathbb{N} = \{1,2,..., \infty \} \smallsetminus \{0\} \subset \mathbb{Z}
\end{align}
$
con $X =$ "Prova di primo successo"

![GraficoVAGeometrica.png](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoVAGeometrica.png)

```


```ad-example
title: Esempio - Prova di primo successo

Sia $S$ il successo con $\mathcal{P}(S) = p \in [0,1]$

$
\begin{align}
\mathcal{P} (X =x) &= \mathcal{P}(\text{Probabilità di avere successo alla prova x}) \\
&= \mathcal{P}(\overline{S} \cap \overline{S} \cap ... \cap \overline{S} \cap S) = \\
&= \mathcal{P}(\overline{S}) \cdot \cdot \cdot \mathcal{P}(\overline{S})\mathcal{P}(S) = (1-p)^{x-1}p
\end{align}
$

```

#### Media VA Geometrica

```ad-Teo
title: Media VA Geometrica

Sia $X \sim \mathrm{Geo}(p)$ con $p \in [0,1]$
La [[#Media di variabili aleatorie Discrete]]  per la [[#Variabile Aleatoria Geometrica]] è: 
$
EX = \frac{1}{p}
$
___
**Dimostrazione**:

Per definizione la [[#Media di variabili aleatorie Discrete]] è 
$
\begin{align}
EX = \sum\limits_{x=1}^{\infty} x \mathcal{P}(X=x)
&= \sum\limits_{x=1}^{\infty}x(1-p)^{x-1}p = \\
&= p \sum\limits_{x=1}^{\infty} x q^{x-1} = \\
&= p \sum\limits_{x=1}^{\infty} \frac{d}{dq}q^{x} = \\
&= p \frac{d}{dq} \sum\limits_{x=1}^{\infty} q^{x} = \\
&= p \frac{d}{dq} \left(\sum\limits_{x=0}^{\infty} q^{x} - q^{0} \right)
\end{align}
$

Riconosco la serie geometrica $\sum\limits_{x=0} q^{x}$ che converge a $\frac{1}{1-q}$.

$
\begin{align}
&= p \frac{d}{dq} \left(\sum\limits_{x=0}^{\infty} q^{x} - q^{0} \right) = \\
&= p \frac{d}{dq} \left( \frac{1}{1-q} - q^{0} \right) = \\
&= p \frac{d}{dq} \left( \frac{1}{1-q} - 1 \right) = \\
&= p \frac{d}{dq} \left( \frac{1}{1-q} - 1 \right) = \\
&= -p (1-q)^{-2}= \\
&= \frac{p}{(1-q)^{2}} = \frac{p}{p^{2}}= \frac{1}{p}\\
\end{align}
$
c.v.d.

```


	Dove va a finire il meno?



```ad-example

Una ditta produce lampadine con vita media: 70 accensioni
Una volta installata la lampadina "L" (presa a caso), calcolare $\mathcal{P}$("si accende").

___
Sia $S$ l'evento: "si accende"
Sia

$
\overline S \cap \overline S \cap ... \cap \overline S \cap S \,\,\,\,\,\,\,\, \mathcal{P}(S) = q
$
e
$
S \cap S \cap ... \cap S \cap \overline S \,\,\,\,\,\,\,\, \mathcal{P}(\overline S) = p
$
$
EX = \frac{1}{q}
$
La media è uguale a 1 fratto la "probabilità che si accenda dopo un certo numero di volte in cui non si è accesa". Il problema richiede la probabilità dell'evento complementare.

$
\begin{align}
\mathcal{P}(S) &= 1- \mathcal{P}(\overline S) =  \\
&= 1- \frac{1}{EX} = \\
&= 1- \frac{1}{70} = \frac{69}{70}
\end{align}
$

```


#### Varianza VA Geometrica

```ad-Teo
title: Varianza VA Geometrica

Sia $X \sim \mathrm{Geo}(p)$ con $p \in [0,1]$
La [[#Varianza]]  per la [[#Variabile Aleatoria Geometrica]] è: 
$
\mathrm{Var}(X) = \frac{1-p}{p^{2}}
$

```

### Variabile Aleatoria di Poisson

```ad-Definizione
title: VA di Poisson (discreta)

$X \sim \text{Pois}(\lambda)$ con $\lambda > 0$
$
\begin{align}
\mathcal{P}(X=x) = e^{-\lambda}\frac{\lambda^{x}}{x!} && x \in \mathbb{N}_{0} = \mathbb{N} \cup \{0\}
\end{align}
$

Dato il tasso di successo $\lambda$, $X$ è il numero di successo nell'intervallo unitario.

![GraficoDensitàPoissonBar.png](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoDensit%C3%A0PoissonBar.png)

```

##### Teoria delle code

La Variabile di Poisson trova applicazione nella teoria delle code. Il Tasso $\lambda$ è infatti il numero di successi medio in un dato intervallo di tempo. Ad esempio: il numero di persone che entrano in banca tra le 10:00 e le 10:01 in media.


#### Media VA di Poisson

```ad-Teo
title: Media VA di Poisson

Per $X \sim \mathrm{Pois}(\lambda)$
$
EX = \lambda
$
___

**Dimostrazione**:

$
\begin{align}
EX &= \sum\limits_{x\ge0} x \mathcal{P}(X=x) = \\
&= \sum\limits_{x\ge0} x e^{-\lambda} \frac{\lambda^{x}}{x!} = \\
&= \sum\limits_{x\ge0} \not x e^{-\lambda} \frac{\lambda^{x}}{ \not x!} = \\
&= e^{-\lambda} \sum\limits_{x\ge1}  \frac{\lambda^{x}}{ (x-1)!} = \\
&= e^{-\lambda} \sum\limits_{x-1\ge0}  \frac{\lambda^{x-1+1}}{ (x-1)!} = \\
\end{align}
$

Chiamo $x-1 = s$:

$
\begin{align}
&= e^{-\lambda} \sum\limits_{x-1\ge0}  \frac{\lambda^{x-1+1}}{ (x-1)!} = \\
&= e^{-\lambda} \sum\limits_{s\ge0}  \frac{\lambda^{s+1}}{ (s)!} = \\
&= \lambda e^{-\lambda} \sum\limits_{s\ge0}  \frac{\lambda^{s}}{ (s)!} = \\
\end{align}
$

In cui riconosco la serie di Taylor Mac-Laurin che converge a $e^{\lambda}$.


$
\begin{align}
&= e^{-\lambda} \sum\limits_{s\ge0}  \frac{\lambda^{s}}{ (s)!} = \\
&= \lambda e^{-\lambda} e^{\lambda} = \lambda
\end{align}
$
*c.v.d.*

```


```ad-example

Da precedenti osservazioni in una banca, si è visto che arrivano mediamente 6 clienti al minuto.

Calcolare
$
\mathcal{P}(\text{"Arrivano 2 clienti tra 12:00 e 12:01"})
$
___

Il tasso di arrivo  è di $6$ clienti al minuto. $\Longrightarrow \lambda = 6 = EX$, dove $EX =$ "il numero di arrivi al minuto".

Di fatto sto chiedendo $\mathcal{P}(X=2)$.

$
\mathcal{P}(X=2) = e^{-6}\frac{6^{2}}{2!}
$

```

#### Varianza VA Poisson
```ad-Teo
title: Varianza VA di Poisson

Sia $X \sim \mathrm{Pois}(\lambda)$ con $\lambda > 0$
La [[#Varianza]]  per la [[#Variabile Aleatoria di Poisson]] è: 
$
\mathrm{Var}(X) = \lambda
$

```

# Variabili aleatorie CONTINUE
Nel mondo reale molte variabili sono **continue** (il tempo ad esempio). Poi nei fatti noi non siamo in grado di misurare le cose in modo continuo e quindi le **osservazioni** saranno **PUNTUALI (discrete).**

Per le variabili aleatorie continue diremo che:
$$
X \sim f_{X} \in C(\mathbb{R})
$$
Ci interessa quindi conoscere solo la <mark style="background: #BBFABBA6;">densità continua</mark>

### Densità continua

```ad-Definizione
title: Densità Continua ($f$)

Sia $f$ una funzione continua. $f: \mathbb{R} \to \mathbb{R}$

Definisco il: 
<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/universita/triennale/2-anno/2-semestre/probabilita-e-statistica/appunti/03-variabili-aleatorie/#supporto" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



#### Supporto

Il supporto di $f$ è l'insieme delle $x$ per cui $f(x)$ è diversa da $0$.
$
\text{Supp}(f) = \{ x \in \mathbb{R}: f(x) \ne 0 \} \subset \text{Dom}(f)
$
Inoltre, sia $X \sim f_{X} \Longrightarrow \text{Supp}(f_{X}) = \text{Supp}(X)$.
Dove
$
\text{Supp}(X) = \{\text{insieme dei valori che X può assumere}\}
$


</div></div>


```

#### Condizioni necessarie e sufficienti per densità continua

```ad-Teo
title: CNS per densità continua

1. $f_{X} \ge 0$
2. $\int_\mathbb{R} f_{X}(x) \, dx = 1$
La funzione continua soddisfa le proprietà 1. e 2. **se e solo se** è una densità continua di qualche variabile aleatoria $X$

```

#### Supporto

Il supporto di $f$ è l'insieme delle $x$ per cui $f(x)$ è diversa da $0$.
$$
\text{Supp}(f) = \{ x \in \mathbb{R}: f(x) \ne 0 \} \subset \text{Dom}(f)
$$
Inoltre, sia $X \sim f_{X} \Longrightarrow \text{Supp}(f_{X}) = \text{Supp}(X)$.
Dove
$$
\text{Supp}(X) = \{\text{insieme dei valori che X può assumere}\}
$$

## Tipi di variabili aleatorie continue

- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile aleatoria Uniforme\|#Variabile aleatoria Uniforme]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Esponenziale\|#Variabile Aleatoria Esponenziale]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Gamma\|#Variabile Aleatoria Gamma]]
- [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Normale o Gaussiana\|#Variabile Aleatoria Normale o Gaussiana]]

### Variabile aleatoria Uniforme

```ad-Definizione
title: VA Uniforme (Continua) - $X \sim \mathrm{Unif}(a,b)$


$X \sim \mathrm{Unif}(a,b)$ con $a < b, (a,b) \in \mathbb{R}$

dove la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Densità continua\|#Densità continua]] è 

$
f_{X}(x) = \frac{1}{b-a} 1_{(a,b)}(x) = 
\begin{cases}
0 & x \notin (a,b) \\
\frac{1}{b-a} & x \in (a,b)
\end{cases}
$

dove $1_{E}(x)$, con $E$ un qualche insieme, è detta **funzione indicatrice** e vale:

$
1_{E}(x) = 
\begin{cases}
0 & x \notin E \\
1 & x\in E
\end{cases}
$
e 
$
\mathrm{Supp}(f_{X}) = (a,b) = \mathrm{Supp}(X)
$


```

Vediamo alcuni casi semplici:

Sia $X \sim \mathrm{Unif}$:
Calcoliamo $\mathcal{P}(X \in (a,b))$

$$
\begin{align}
\mathcal{P}(X \in (a,b)) &= \int_{(a,b)} f_{X}(x) \, dx = \\
&= \int_{a}^{b} f_{X}(x) \, dx = \\
&= \int_{a}^{b} \frac{1}{(b-a)} 1_{(a,b)}(x) \, dx = \\
&= \int_{a}^{b} \frac{1}{(b-a)} \, dx = \frac{b-a}{b-a} = 1
\end{align}
$$

___

Calcoliamo $\mathcal{P}(X \in [a,b])$ per capire che differenza c'è tra usare un intervallo aperto e uno chiuso.

$$
\begin{align}
\mathcal{P}(X \in [a,b]) &= \int_{[a,b]} f_{X}(x) \, dx = \\
&= \int_{\{a\}} f_{X}(x) \, dx + \int_{a}^{b} f_{X}(x) \, dx + \int_{\{b\}} f_{X}(x) \, dx= \\
&= 0 + \int_{a}^{b} \frac{1}{(b-a)} 1_{(a,b)}(x) \, dx + 0= \\
&= \int_{a}^{b} \frac{1}{(b-a)} \, dx = \frac{b-a}{b-a} = 1
\end{align}
$$
___

Calcoliamo $\mathcal{P}(X< a)$
  $$
\begin{align}
\mathcal{P}(X < a)  = \mathcal{P}(x \in (- \infty, a) &= \int_{(-\infty,a)} f_{X}(x) \, dx = \\
&= \int_{-\infty}^{a} f_{X}(x) \, dx = \\
&= \int_{-\infty}^{a} \frac{1}{(b-a)} 1_{(a,b)}(x) \, dx = 0
\end{align}
$$

Essendo $x$ sempre fuori dall'intervallo $(a,b)$ e quindi la funzione indicatrice sarà sempre nulla


#### Media VA Uniforme

```ad-Teo
title: Media VA Uniforme

Sia $X \sim \mathrm{Unif}(a,b)$ con $a,b \in \mathbb{R}$
La [[#Media di variabili aleatorie Continue]]  per la [[#Variabile aleatoria Uniforme]] è: 
$
\mathrm{E}(X) = \frac{a+b}{2}
$
```

#### Varianza VA Uniforme

```ad-Teo
title: Varianza VA Uniforme

Sia $X \sim \mathrm{Unif}(a,b)$ con $a,b \in \mathbb{R}$
La [[#Varianza]]  per la [[#Variabile aleatoria Uniforme]] è: 
$
\mathrm{Var}(X) = \frac{(b-a)^{2}}{12}
$
```



### Variabile Aleatoria Esponenziale

> Si ritrova spesso nel descrivere la quantità di tempo che passa prima che uno specifico evento accada. 

Ad esempio, il tempo, a partire da ora, primo che ci sia un terremoto o che scoppi una nuova guerra, o che una chiamata ricevuta risulti da qualcuno che ha sbagliato numero. 

```ad-Definizione
title: VA Esponenziale (continua): $X \sim \mathrm{Exp}(\lambda)$


$X \sim \mathrm{Exp}(\lambda), \quad \lambda > 0$

La [[#Densità continua]] è

$
f_{X}(x) = \lambda e^{-\lambda x} 1_{(0,\infty) }(x) =
\begin{cases}
0 & x < 0 \\
\lambda e^{-\lambda x} & x>0
\end{cases}
$

e 
$\mathrm{Supp}(X) = \mathrm{Supp}(f_{X}) = (0, \infty)$

```

Vediamo alcuni casi semplici:

Calcoliamo $\mathcal{P}(X < 1)$.
Ricordo $x>0$ evento certo in $\Omega$.

$$
\begin{align}

\mathcal{P} (X<1) = \mathcal{P}(x<1, x>0) &= \mathcal{P}(0<x<1) = \\

&= \int_{0}^{1} \lambda e^{-\lambda x}1_{(0, \infty)}(x) \, dx = \\

&= \left[ -e^{-\lambda x} \right]_{0}^{1}  = -e^{-\lambda} + 1

\end{align}
$$
___

```ad-example


Calcolare $\mathcal{P}(1<X<\frac{3}{2})$

$
\begin{align}

\mathcal{P} (1<X<\frac{3}{2}) &= \int_{1}^{\frac{3}{2}} \lambda e^{-\lambda x}1_{(0, \infty)}(x) \, dx = \\

&= \left[ -e^{-\lambda x} \right]_{1}^{\frac{3}{2}}  = -e^{- \frac{3}{2}\lambda} + e^{-\lambda}

\end{align}
$

```

```ad-example

Calcolare $\mathcal{P}(-5<X<5)$

$
\begin{align}
\mathcal{P} (1<X<\frac{3}{2}) &= \mathcal{P}(X \in [-5,5])= \\
&= \mathcal{P}(X \in [-5,5], X \in (0, \infty)) = \\
&= \mathcal{P}(X \in [-5, 5] \cap (0, \infty)) = \\
&= \mathcal{P}(X \in (0,5]) = \\
&= \int_{0}^{5} \lambda e^{-\lambda x} \, dx = \\
&= \left[ -e^{-\lambda x} \right]_{0}^{5}  = 1 - e^{- 5 \lambda}
\end{align}
$

```


#### Media VA Esponenziale

```ad-Teo
title: Media VA Esponenziale

Sia $X \sim \mathrm{Exp}(\lambda)$ con $\lambda > 0$
La [[#Media di variabili aleatorie Continue]]  per la [[#Variabile Aleatoria Esponenziale]] è: 
$
\mathrm{E}(X) = \frac{1}{\lambda}
$
```

#### Varianza VA Esponenziale

```ad-Teo
title: Varianza VA Esponenziale

Sia $X \sim \mathrm{Exp}(\lambda)$ con $\lambda > 0$
La [[#Varianza]]  per la [[#Variabile Aleatoria Esponenziale]] è: 
$
\mathrm{Var}(X) = \frac{1}{\lambda^{2}}
$
```


### Variabile Aleatoria Gamma

```ad-Definizione
title: VA Gamma (Continua): $X \sim \mathrm{Gamma}(\lambda, \nu)$

$X \sim \mathrm{Gamma}(\lambda, \nu)$

La [[#Densità continua]] è 
$
f_{X}(x) = \frac{\lambda^{\nu}}{\Gamma(\nu)}x^{\nu - 1}e^{-\lambda x} 1_{(0, \infty)}(x)
$
e [[#Supporto]]:

$
\mathrm{Supp}(f_{X}) = \mathrm{Supp}(X) = (0, \infty)
$

e dove $\Gamma(\lambda)$ è la [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Funzione Gamma\|#Funzione Gamma]] calcolata in $\lambda$

```

Verifichiamo che  $\mathcal{P} (X \in (0, \infty)) = 1$
Verifico a partire dalla densità $f_X$.

$$
\begin{align}

\mathcal{P} (X \in (0, \infty)) &= \int_{(0,\infty)}f_{X}(x) \, dx = \\

&= \int_{0}^{\infty} \frac{\lambda^{\nu}}{\Gamma(\nu)} x^{\nu - 1} e^{-\lambda x} 1_{(0, \infty)} \, dx  \\
\end{align}
$$

Applico il seguente cambio di variabile:
$$
\lambda x = q
$$
Da cui $x = \frac{q}{\lambda}$ e $dx = \frac{dq}{\lambda}$

ottengo così
$$
\int_{0}^{\infty} \frac{\lambda^{\nu}}{\Gamma(\nu)} x^{\nu - 1} e^{-\lambda x} 1_{(0, \infty)} \, dx 

= \frac{\not {\lambda^{\nu}}}{\Gamma(\nu)} \frac{1}{\not{\lambda^{\nu - 1}}} \frac{1}{\not \lambda} \int_{0}^{\infty} q^{\nu-1}e^{-q} \, dq
$$

in cui riconosco $\int_{0}^{\infty}q^{\nu-1}e^{-q} \, dq = \Gamma(\nu)$ per cui ottengo

$$
\frac{\Gamma(\nu)}{\Gamma(\nu)} = 1
$$


##### Funzione Gamma

```ad-Definizione
title: Funzione Gamma ($\Gamma$)

La funzione Gamma è la funzione:
$
\Gamma(z) = \int_{0}^{\infty}s^{z-1}e^{-s}ds, \,\,\,\, z > 0
$

```

###### Proprietà della funzione gamma

```ad-Teo
title: Proprietà della funzione Gamma

La funzione gamma gode delle seguenti proprietà:

- $\Gamma(z+1)$ = $z\Gamma(z)$
- $\Gamma\left(\frac{1}{2}\right)= \sqrt{\pi}$
- $\Gamma(z+1) = z!$ se $z \in \mathbb{N}$
- $\Gamma(2) = 1$ 

___

**Dimostrazione**:

Cominciamo da $\Gamma(z+1) = z \Gamma(z)$

$
\begin{align}
z \Gamma(z) = \int_{0}^{\infty} z s^{z-1} e^{-s}   \, ds &\stackrel{\text{Per parti}} {=} \left[z \frac{1}{z} s^{z}e^{-s} \right]_{0}^{\infty} - \int_{0}^{\infty} z \frac{1}{z} (-e^{-s})s^{z}  \, ds = \\
&= \underbrace{ [ \infty^{z}e^{-\infty} - 0^{z}e^{-0} ]}_{\to 0} + \underbrace{\int_{0}^{\infty} s^{z}e^{-s}  \, ds }_{= \Gamma(z+1)} = \Gamma(z+1)
\end{align}
$
*c.v.d*

Da questo, segue immediatamente la terza affermazione (per $z \in \mathbb{N}$):
$
\Gamma(z+1) = z\Gamma(z) = z(z-1) \Gamma(z-1) ...
$
Continuando fino ad ottenere 
$
\Gamma(z+1) = z!
$
*c.v.d*
```


#### Media VA Gamma

```ad-Teo
title: Media VA Gamma
$
EX = \frac{\nu}{\lambda}
$
___
**Dimostrazione**


$
\begin{align}
EX &= \int_{\mathbb{R}} x f_{X} (x) \, dx =  \\
&= \int_{\mathbb{R}}  x \frac{\lambda^{\nu}}{\Gamma(\nu)}x^{\nu - 1}e^{-\lambda x} 1_{(0, \infty)}(x) \, dx  \\
&= \frac{\lambda^{\nu}}{\Gamma(\nu)} \int_{0}^{\infty} x^{\nu}e^{-\lambda x} \,dx =
\begin{bmatrix}
\lambda x = y \\
x = \lambda^{-1} y \\
dx = \lambda^{-1}dy
\end{bmatrix}   \\
&= \frac{\lambda^{\nu}}{\Gamma(\nu)} \int_{0}^{\infty}\lambda^{-\nu} y^{\nu}e^{-y} \lambda^{-1} \,dx = \\
&= \frac{\lambda^{-1}}{\Gamma(\nu)} \int_{0}^{\infty} y^{\nu}e^{-y} \,dx
\end{align}
$
Da $\Gamma(\nu+1) = \nu \Gamma(\nu)$ per [[#Proprietà della funzione gamma]] si ha che $\frac{1}{\Gamma(\nu)} = \frac{\nu}{\Gamma(\nu + 1)}$. Allora
$
\begin{align}
&= \frac{\lambda^{-1}}{\Gamma(\nu)} \int_{0}^{\infty} y^{\nu}e^{-y} \,dx =  \\
&= \lambda^{-1}\frac{\nu}{\Gamma(\nu+1)} \underbrace{\int_{0}^{\infty} y^{\nu}e^{-y} \,dx}_{= \Gamma(\nu+1)} = \frac{\nu}{\lambda}
\end{align}
$
*c.v.d.*
```


#### Varianza VA Gamma

```ad-Teo
title: Varianza VA Gamma

$
Var(X) = \frac{\nu}{\lambda^{2}}
$

```

### Variabile Aleatoria Normale o Gaussiana

> La Variabile Normale si ritrova in moltissime applicazioni pratiche. È stata introdotta inizialmente come approssimazione di una [[#Variabile Aleatoria Binomiale]] per grandi valori del parametro $n$.
   È usata, ad esempio, come distribuzione:
> - Dell'altezza di una persona
> - Dell'errore nella misura di una quantità fisica



```ad-Definizione
title: VA Normale o Gaussiana (Continua): $X \sim N(\mu, \sigma^{2})$

$X \sim N(\mu, \sigma^{2})$ con $\mu \in \mathbb{R}, \sigma^{2} > 0$ 

La [[#Densità continua]] è
$
f_{X}(x) = \frac{1}{\sqrt{2 \pi \sigma^{2}}}e^{\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$
con $x \in \mathbb{R}$.

Il [[#Supporto]]:

$
\mathrm{Supp}(f_{X}) = \mathrm{Supp}(X) = \mathbb{R}
$

___

Per $\mu = 0$ e $\sigma^{2}=1$ si parla di **Normale Standard**: $X \sim N(0, 1)$

![GraficoDistribuzioneNormale 2.png](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoDistribuzioneNormale%202.png)

```

Verifichiamo che $\mathcal{P}(X \in \mathbb{R}) = 1$

$$
\begin{align}
\mathcal{P}(X \in \mathbb{R}) = \int_{\mathbb{R}} f_{X}(x)\, dx &= \int_{\mathbb{R}}  \frac{e^{- \frac{(x-\mu)^{2}}{2 \sigma^{2} }}}{\sqrt{2 \pi \sigma^{2}}} \, dx  =  \\
&= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \int_{- \infty}^{\infty} e^{- \frac{(x-\mu)^{2}}{2 \sigma^{2} }} \, dx  = \begin{bmatrix}
y = x-\mu \\
x = y+\mu \\
dx = dy
\end{bmatrix} =  \\
&= \frac{2}{\sqrt{2 \pi \sigma^{2}}} \int_{0}^{\infty} e^{- \frac{y^{2}}{2 \sigma^{2} }} \, dy  = \begin{bmatrix}
\omega = y^{2} \\
y = \omega^{\frac{1}{2}} \\
dy = \frac{1}{2}\omega^{- \frac{1}{2}}
\end{bmatrix} =   \\
&= \frac{\cancel{2}}{\sqrt{2 \pi \sigma^{2}}} \int_{0}^{\infty} e^{- \frac{\omega}{2 \sigma^{2} }} \frac{1}{\cancel{2}} \omega^{\frac{1}{2}-1}\, d \omega
\end{align}
$$
Moltiplico e divido per la quantità
$$
\frac{\Gamma\left( \frac{1}{2} \right)}{\left( \frac{1}{2 \sigma^{2}} \right)^{\frac{1}{2}}}
$$
ottengo

$$
\begin{align}
\mathcal{P}(X \in \mathbb{R}) &= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \frac{\Gamma\left( \frac{1}{2} \right)}{\left( \frac{1}{2 \sigma^{2}} \right)^{\frac{1}{2}}}\int_{0}^{\infty} \frac{\Gamma\left( \frac{1}{2} \right)}{\left( \frac{1}{2 \sigma^{2}} \right)^{\frac{1}{2}}} e^{- \frac{\omega}{2 \sigma^{2} }}  \omega^{\frac{1}{2}-1}\, d \omega
\end{align}
$$
Noto che l'integrale è diventato quello per la probabilità $\mathcal{P}(X \in \mathbb{R})$ per $X \sim \mathrm{Gamma}\left( \frac{1}{2 \sigma^{2}}, \frac{1}{2} \right)$ che abbiamo già dimostrato essere uguale a 1 in [[Università/Triennale/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie#Variabile Aleatoria Gamma\|#Variabile Aleatoria Gamma]].
Per cui rimane
$$
\begin{align}
\mathcal{P}(X \in \mathbb{R}) &= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \frac{\Gamma\left( \frac{1}{2} \right)}{\left( \frac{1}{2 \sigma^{2}} \right)^{\frac{1}{2}}}
\end{align}
$$
dove $\Gamma\left( \frac{1}{2} \right) = \sqrt(\pi)$ per le **proprietà** della [[#Funzione Gamma]]. Per cui
$$
\begin{align}
\mathcal{P}(X \in \mathbb{R}) &= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \frac{\sqrt{\pi}}{\sqrt{\frac{1}{2 \sigma^{2}}}} =  \\
&= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \sqrt{2 \pi \sigma^{2}} = 1
\end{align}
$$


#### Media VA Normale

```ad-Teo
title: Media VA Normale

$
E(X) = \mu 
$

```

#### Varianza VA Normale

```ad-Teo
title: Varianza VA Normale



$
\mathrm{Var}(X) = \sigma^{2}
$
___
**Dimostrazione**

$
\begin{align}
\mathrm{Var}(X) &= E(X-\mu)^{2} = \\
&= \int_\mathbb{R} (x-\mu)^{2}f_{X}(x) \, dx = \\
&= \int_\mathbb{R} (x-\mu)^{2}\frac{e^{\frac{(x-\mu)^{2}}{2 \sigma^{2}}}}{\sqrt{2 \pi \sigma^{2}}} \, dx = 
\begin{bmatrix} x-\mu = y \\ x = y+\mu \\ dx = dy \end{bmatrix} =\\
&= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \int_\mathbb{R}y^{2} e^{-\frac{y^{2}}{2\sigma^{2}}} \, dy = \begin{bmatrix} y^{2} = t \\ y = t^{\frac{1}{2}} \\ dy = \frac{1}{2}t^{\frac{1}{2}-1} dt \end{bmatrix} =\\
&= \frac{\not 2}{\sqrt{2 \pi \sigma^{2}}} \int_\mathbb{R}t e^{-\frac{t}{2\sigma^{2}}} \frac{1}{\not 2}t^{\frac{1}{2}-1}\, dt = \\
&= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \int_\mathbb{R}t^{\frac{1}{2}} e^{-\frac{t}{2\sigma^{2}}} \, dt = \begin{bmatrix} \frac{t}{2 \sigma^{2}} = l \\ t = 2 \sigma^{2}l \\ dt = 2 \sigma^{2} dl \end{bmatrix} = \\
&= \frac{1}{\sqrt{\not 2 \pi \not \sigma^{2}}} \int_\mathbb{R}(\not 2 \not \sigma^{2}l)^{\frac{1}{2}} e^{-l} 2 \sigma^{2}\, dl = \\
&= \frac{2 \sigma^{2}}{\sqrt{\pi}} \int_{0}^{\infty} l^{\frac{1}{2}}e^{-l} \, dl = \\
&= \frac{2 \sigma^{2}}{\sqrt{\pi}} \int_{0}^{\infty} l^{\frac{3}{2}-1} e^{-l} \, dl = \\
&= \frac{2 \sigma^{2}}{\sqrt{\pi}} \Gamma \left(\frac{3}{2} \right) = \frac{2 \sigma^{2}}{\sqrt{\pi}} \frac{1}{2} \sqrt{\pi} = \sigma^{2}
\end{align}
$
*c.v.d.*

```


# Funzione di Ripartizione
#ArgomentoFondamentale 

```ad-Definizione
title: Funzione di Ripartizione ($F_{X}(x)$)

Data $X \sim f_{X}$ si dice **funzione di ripartizione di** $X$ la funzione così definita:
$
F_{X}(x) = \mathcal{P}(X \le x) = \int_{-\infty}^{x} f_{X}(y) \, dy 
$

```

Dall'analisi, per il [[Teorema Fondamentale del Calcolo Integrale\|Teorema Fondamentale del Calcolo Integrale]] abbiamo
$$
f_{X}(x) = \frac{d}{dx} F_{X}(x)
$$

### Proprietà della funzione di ripartizione

```ad-Teo
title: Proprietà Funzione di Ripartizione

$
\begin{align}
1. &\quad \mathcal{P}(X \in (-\infty,x]) = \mathcal{P}(X \le x) = F_{X}(x) \\
2. &\quad \mathcal{P}(X \in (x_{1}, x_{2}]) = \mathcal{P}(x_{1}<X\le x_{2}) = F_{X}(x_{2}) - F_{X}(x_{1}) \\
3. &\quad \mathcal{P}(X \le x_{2}) = F_{X}(x_{1}) + \mathcal{P}(x_{1}<X<x_{2}) \\
4. &\quad \lim_{x \to \infty} F_{X}(x) = 1 \qquad \qquad \lim_{x \to - \infty}F_{X}(x) = 0 \\
5. &\quad \lim_{x \to x_{0}^{+}} F_{X}(x) = F_{X}(x_{0}) \qquad \text{(Continuità)}
\end{align}
$
```

## Trasformazioni di variabili aleatorie

È data una [[#3. Variabili Aleatorie|variabile aleatoria]] $X \sim f_{X}(x)$. 
Si ha inoltre che 
$$
Y = g(X)
$$
Vogliamo caratterizzare $Y$, ossia trovarne la [[#Densità continua]] o densità discreta.

```ad-note
title: Osservazione

Se $X$ è discreta, allora anche $g$ è una funzione discreta.

```

```ad-example
title: Esempio


Sia $X \sim \mathrm{Exp}(\lambda)$ e $Y = \sqrt{X}$. Caratterizzare $Y$ (trovarne la densità).

Alcune considerazioni:
- $X$ è continua
- $\sqrt{}$ è continua
Anche $Y$ è continua

Per l'esponenziale, $X \in (0, \infty)$ per cui $Y = g(X) \in (0, \infty)$

$
\mathrm{Supp}(f_{Y}) = \mathrm{Supp}(Y) = (0, \infty)
$
Come primo passo devo trovare $F_{Y}$ per poi derivare e cercare $f_{Y}$.


$
\begin{align}
F_{Y}(y) &= \mathcal{P}(Y \le y) =  \\
&= \mathcal{P}(\sqrt{X} \le y) =  \\
&= \mathcal{P}(X\le y^{2}) =  \\
&= \int_{-\infty}^{y^{2}} f_{X}(t)   \, dt =  \\
&= \int_{-\infty}^{y^{2}} \lambda e^{-\lambda x} 1_{(0, \infty)}(t)   \, dt =   \\
&= \int_{0}^{y^{2}} \lambda e^{-\lambda x} \, dt =    \\
&= 1 - e^{- \lambda y^{2}}
\end{align}
$
Per $y \in (0, \infty)$

Per cui la [[#Densità continua]] è data da
$
\begin{align}
f_{Y}(y) &= \frac{d}{dy} F_{Y}(y) = \\
&= 2 \lambda y e^{- \lambda y^{2}} \qquad y > 0 
\end{align}
$
In $\mathbb{R}$:
$
f_{Y}(y) = \begin{cases}
0, &\quad y\le0 \\
2 \lambda y e^{- \lambda y^{2}}, &\quad y>0
\end{cases}
$
verificato che la funzione sia continua in 0. Ossia
$
f_{Y}(y) = 2 \lambda y e^{- \lambda y^{2}} 1_{(0,\infty)}(y)
$

```


![Drawing 2023-04-08 17.47.02 2. Misure e Probabilità - Ricapitolandolo.excalidraw.png](/img/user/Excalidraw/Drawing%202023-04-08%2017.47.02%202.%20Misure%20e%20Probabilit%C3%A0%20-%20Ricapitolandolo.excalidraw.png)