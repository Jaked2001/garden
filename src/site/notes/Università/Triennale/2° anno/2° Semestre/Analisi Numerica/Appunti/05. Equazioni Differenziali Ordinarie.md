---
{"dg-publish":true,"permalink":"/universita/triennale/2-anno/2-semestre/analisi-numerica/appunti/05-equazioni-differenziali-ordinarie/"}
---


❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗

- [[#Stabilità One-Step]]
- [[#Collocazione]]





___
[[_Giornaliera/2023-04-12\|2023-04-12]]
___
# 5. Soluzione numerica di equazioni differenziali ordinarie

## Introduzione
Molti fenomeni possono essere descritti con modelli matematici consistenti di *sistemi di equazioni differenziali ordinarie (EDO) del primo ordine*:

$$
\begin{cases}

y_{1}'(x) &= f_{1}(x, y_{1}(x), ..., y_{n}(x)) \\
y_{2}'(x) &= f_{2}(x, y_{1}(x), ..., y_{n}(x)) \\
\,\,\,\,\,\,\vdots \\
y_{n}'(x) &= f_{n}(x, y_{1}(x), ..., y_{n}(x))

\end{cases}
$$

soggette ad *assegnate condizioni iniziali:*

$$
\begin{cases}
y_{1}(x_{0}) &= y_{10} \\
y_{2}(x_{0}) &= y_{20} \\
\,\,\,\,\,\vdots \\
y_{n}(x_{0}) &= y_{n0} \\
\end{cases}
$$

### Esempi
#### Pendolo
```ad-example
title: Pendolo

Il moto di un oscillatore armonico (come può essere un pendolo) senza attrito, è descritto dall'equazione:

$
\ddot \theta(t) - \frac{g}{L} \sin(\theta (t)) = 0
$

Se vogliamo risolvere questo problema in modo analitico, siamo costretti ad usare l'approssimazione delle piccole oscillazioni per cui:

$
\theta (t) \sim 0 \Longrightarrow \sin(\theta(t)) \sim \theta(t)
$

da cui l'equazione diventa:

$
\ddot \theta(t) - \frac{g}{L} \theta (t) = 0
$

di cui appunto sappiamo trovare la soluzione. 

Per risolvere però il problema reale, siamo costretti a non usare le piccole oscillazioni. In questo caso l'equazione risulta non lineare e non ne conosciamo la soluzione esplicita. Dobbiamo quindi avvalerci di un metodo numerico.

In ogni caso, per la soluzione sono necessarie le condizioni iniziali che permettano di scrivere il [[#Problema di Cauchy]]:

$
\begin{cases}
\ddot \theta(t) - \frac{g}{L} \sin(\theta (t)) = 0 \\
\theta(t_{0}) = \theta_{0} \, ,\,\,\,\,\,\, \dot \theta (t_{0}) = v_{0}
\end{cases}
$


```

#### Modello Preda-Predatore
```ad-example
title: Modello preda-predatore (Larka-Volterra)


È un modello utile a descrivere l'andamento delle popolazioni di due specie di cui una è predatrice dell'altra. È stato sviluppato da Larka e Volterra. 

Si definiscono 2 specie: $P$ e $N$. La variazione (derivate) delle popolazioni è descritta dal seguente sistema:

$
\begin{cases}

\dot P(t) &= k_{1} P(t) - \frac{1}{\mu_{2}} P(t)N(t) \\

\dot N(t) &= -k_{2} N(t) + \frac{1}{\mu_{1}} P(t)N(t) \\

P(t_{0}) &= P_{0}, \,\,\,\,\,\,\,\,\,\,\, N(t_{0}) = N_{0}

\end{cases}
$

Le prede ($P$) aumentano esponenzialemente in assenza di predatori ($N$).

Viceversa, i predatori diminuiscono esponenzialmente in assenza di prede.

La relazione è esponenziale: infatti  risolvendo $\dot P = k_{1}P$ si otterrebbe proprio $P = e^{k_{1}t}$.

Le prede poi diminuiscono in presenza dei predatori e questi ultimi aumentano in presenza di prede.

Questo si tratta di un modello non lineare costituito da un sistema di 2 EDO a 2 incognite.
Non sappiamo trovare la soluzione esplicita anche se in questo caso si può dimostrare essere periodica.


```

#### Crescita logistica
```ad-example
title: Crescita logistica

Il modello:

$
\begin{cases}

\dot P (t) &= b P(t) - k P^{2} (t) \\

P(t) &= P_{0} 

\end{cases}
$

![Equazione logistica - Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Logistic-curve.png/290px-Logistic-curve.png)


```


### Definizioni
#### Problema di Cauchy

```ad-Definizione
title: Problema di Cauchy

Il problema di Cauchy è un problema differenziale nella forma:

$
\begin{cases}

y'(t) &= f(x, y(t)) \,\,\,\,\, t > t_{0} \\


y(t_{0}) &= y_{0}

\end{cases}
$


```

#### Problema ai limiti
```ad-Definizione
title: Problema ai limiti
Un problema differenziale nella forma:
$
\begin{cases}
y''(x) = f(x, y(x), y'(x)) \qquad a <x< b \\
y(a) = \alpha, \quad y(b) = \beta
\end{cases}
$

```

#### Intervallo di discretizzazione

```ad-Definizione
title: Intervallo di discretizzazione

Si definisce il [[#Problema di Cauchy]] in un intervallo:

$
I = [x_{0}, x_{0} + \beta]
$

nel quale è noto esistere un'unica soluzione $y(x)$.

Si considera una **discretizzazione** di $I$ in sottointervalli di ampiezza $h$, ottenuti con l'introduzione di $n+1$ **nodi** equispaziati. 

- Nodi: $t_{0}, t_{1}, t_{2}, ..., t_{n}=t_{0} + \beta$
- Approssimazioni: $y_{0} = y(t_{0}), y_{1}, y_{2}, ..., y_{n}$

Risulta

$
y_{i} = y(t_{i})
$

Si ha quindi che

$
h = \frac{\beta}{n}
$

e

$
t_{i} = t_{0} + ih
$

Inoltre, l'errore di troncamento è

$
e_{i} = y(t_{i}) - y_{i}
$



```



#### Funzione Lipschitziana

```ad-Definizione
title: Funzione Lipschitziana

Una funzione $f(t, y)$ si dice **Lipschitziana** in $y$ uniformemente rispetto a $t$ in $D \subseteq \mathbb{R^{2}}$ (Dominio) se $\exists L > 0$ tale che:

$
|f(t, y_{1}) - f(t, y_{2})| \le L |y_{1} - y_{2}| \,\,\,\,\, \forall (t, y_{1}), (t, y_{2}) \in D
$

```

##### Condizione sufficiente Lipschitzianeità

```ad-Teo
title: Teorema

Se $f(t, y)$
- È derivabile
- $\max\limits_{(t,y) \in D} \left| \frac{\partial f}{\partial y}(t, y) \right| = L < + \infty$

**Allora**:

$f$ è Lipschitziana

```

Se $L$ è molto grande, si dice che i problemi sono "*stiff*" e hanno variazioni molto grandi.

#### Esistenza della soluzione One-Step

```ad-Teo
title: Esistenza e unicità della soluzione

Se $f(t, y)$ è [[#Funzione Lipschitziana]] in $S = [t_{0}, t_{0} + \beta] \times (- \infty, + \infty)$

**Allora**

- La soluzione *esiste* ed è *unica*
- Il [[#Problema di Cauchy]] è [[#benposto]]

```


#### Errore di troncamento globale One-Step

```ad-Definizione
title: Errore di troncamento ($e_{i}$)

L'errore di troncamento è

$
e_{i} = y(t_{i}) - y_{i}
$

Dove ricordo essere:
- $y(t_{i})$ = la soluzione esatta nel punto $t_{i}$
- $y_{i}$ = La soluzione approssimata nel punto $t_{i}$


```

#### Errore di troncamento locale One-Step

L'errore di Troncamento locale è

$$
R (t_{i}, h, y) = \frac{h^{2}}{2}y''(\tau_{i}) \,\,\,\,\,\,\, \tau_{i} \in [t_{i}, t_{i+1}]
$$
se
$$
y \in C^{2}[t_{0}, t_{0} + \beta] \Longrightarrow \lim_{h\to 0} \frac{R(t_{i},h,y)}{h} = 0
$$

#### Convergenza One-Step

```ad-Definizione
title: Convergenza

Si dice che il metodo **converge** se


$
\lim_{h\to 0} \max_{0 \le i \le n} |e_{i}| = 0  
$


```

#### Consistenza One-Step
```ad-Definizione
title: Consistenza
Si dice che il metodo è **consistente** se per $y \in C^{2}[t_{0}, t_{0} + \beta]$ allora
$
\lim_{h \to 0} \frac{R(t_{i}, h, y)}{h} = 0
$
```

Se è verificata la consistenza, dato
$$
y(t_{i+1}) = y(t_{i} + h) = y(t_{i}) + h \phi(t_{i}, y(t_{i})) + R(t_{i}, h, y)
$$
Si ha che, essendo l'[[#Errore di troncamento locale]]:
$$
R(t_{i}, h, y) = \lim_{h \to 0} \left(\frac{y(t_{i}+h) - y(t_{i})}{h} - \frac{\cancel{h} \phi(t_{i}, y(t_{i}))}{\cancel{h}} \right) = 0
$$
possiamo scrivere
$$
\begin{align}
\lim_{h \to 0} \frac{y(t_{i}+h)-y(t_{i})}{h} &= \lim_{h \to 0} \phi(t_{i},y(t_{i})) \\
y'(t) &= \lim_{h \to 0} \phi(t_{i}, y(t_{i})) \\
f(t_{i}, y(t_{i})) &= \lim_{h \to 0} \phi(t_{i}, y(t_{i})) 
\end{align}
$$
per cui
$$
f(t_{i}, y(t_{i})) = \lim_{h \to 0} \phi(t_{i}, y(t_{i})) 
$$

#### Stabilità One-Step

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗

## Metodi One-Step

Metodi per la risoluzione di [[#Problema di Cauchy]] che sfruttano un algoritmo nella forma:
$$
\begin{cases}
y_{i+1} = y_{i} + h \phi(t_{i}, y_{i}) \qquad 0 \le i \le n-1 \\
y_{0} = y(t_{0})
\end{cases}
$$
Sono:
- [[#Metodo di Eulero]]
- [[#Metodo di Heun]]
- [[#Metodo di Runge-Kutta]]

### Metodo di Eulero

<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/universita/triennale/2-anno/2-semestre/analisi-numerica/appunti/05-1-metodo-di-eulero/" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">




# Metodo di Eulero
___

Introduzione del metodo


## ☑️ Ipotesi

```ad-tip
title: Ipotesi


```

## Algoritmo

![GraficoEulero.png|350](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Analisi%20Numerica/Appunti/allegati/GraficoEulero.png)

### Discretizzazione

$I = [x_{0}, x_{0} + \beta]$ intervallo di discretizzazione.
- Nodi: $t_{i}= t_{0} + i h \qquad 1 \le i \le n \qquad y_{0} = y(t_{0})$
- Approssimazioni: $y_{i} = y(t_{i})$


Scrivo $y(t)$ come:
$
y(t) = y(t_{0}) + (t-t_{0}) y'(t_{0}) + \frac{(t - t_{0})^{2}}{2}y''(\tau_{0}) \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \text{ con } \tau_{0} \in [t_{0}, t_{1}], \text{ } t_{1}= t_{0} + h
$
Calcolato in $t_{1}$ diventa

$
y(t_{1}) = y(t_{0}) + (t_{1}-t_{0}) y'(t_{0}) + ... + R(t_{1}, h, y)
$

Dove $R(t_{1}, h, y)$ è l'[[#Errore di troncamento locale]].
Essendo, per come è stato [[#Definizioni|definito]] il problema
$
\begin{align}

t_{1}- t_{0} &= h \\

y'(t_{0}) &= f(t_{0}, y(t_{0}))

\end{align}
$

posso scrivere:

$
y(t_{1}) = y(t_{0}) + h f(t_{0}, y(t_{0})) + R(t_{1}, h, y)
$

E posso quindi scrivere l'approssimazione $y_{1}$ come

$
y_{1} = y_{0} + h f(t_{0}, y_{0})
$

```ad-tip
title: Algoritmo

$
\begin{cases}
y_{i+1} &= y_{i} + hf(t_{i}, y_{i}) \\
y_{0} &= y(t_{0})
\end{cases}
$
```



## Errori
### Errore di Troncamento Globale

```ad-Teo
title: Errore di troncamento Globale

$e_{i} = y(t_{i}) - y_{i}$
```

### Errore di troncamento locale
```ad-Teo
title: Errore di troncamento locale Metodo di Eulero
$
R(t_{i}, h, y) = \frac{h^{2}}{2}y'' (\tau_{i}) \qquad \tau_{i} \in [t_{i}, t_{i+1}]
$
```



## Convergenza

### Condizioni di convergenza

NON VISTO
### Ordine di convergenza

```ad-Teo
title: Ordine di Convergenza

Il [[#Metodo di Eulero]] ha **ordine di convergenza** pari a 1:
$
p = 1
$
```


### Stima iterazioni necessarie
NON VISTO

## Criterio di arresto
### Criterio di arresto a posteriori

Posso interrompere l'algoritmo quando l'errore al passo k è minore di una tolleranza scelta $\varepsilon$.
$
|e_{k}| \le \varepsilon
$

## Implementazione in Matlab

Riportare la function per l'applicazione del metodo in matlab

```matlab
function [Ti,Yi] = MetodoEulero(fun, I, y0, n_step)

% Metodo di Eulero

% Calcola la soluzione di una EDO del orimo ordine ai valori iniziali con

% il metodo di Eulero

% Input:

% fun(t,y): termine noto del problema (function)

% I(1:2): Estremi dell'intervallo di integrazione (vettore)

% y0(1): Condizione iniziale

% n_step: Numero passi temporali (scalare)

%

% Output:

% Ti(1:n_step + 1): Nodi di discretizzazione (vettore)

% Yi(1:n_step + 1): Vettore delle approssimazioni (vettore)

t0 = I(1);

tf = I(2);

% Passo di discretizzazione

h = (tf-t0)/n_step;

% Griglia dei nodi: equispaziata

Ti = linspace(t0,tf, n_step + 1);

Ti = Ti'; % Rendo Ti un vettore colonna

% Inizializzazioni

Yi = nan(n_step + 1, 1);

Yi(1) = y0;

% Metodo di Eulero

for i= 1:n_step

Yi(i+1) = Yi(i) + h*fun(Ti(i), Yi(i));

end

end

```


</div></div>




### Metodo di Heun
```ad-tip
title: Algoritmo metodo di Heun

$
\begin{cases}
y_{i+1} = y_{i} + \frac{h}{2} [ K_{1}(t_{i},y_{i}) +  K_{2}(t_{i}, y_{i})] \\
y_{0} = y(t_{0})
\end{cases}
$
dove 
$
\begin{align}
K_{1}(t_{i}, y_{i}) &= f(t_{i}, y_{i}) \\
K_{2}(t_{i}, y_{i}) &= f(t_{i} + h, y_{i} + h K_{1}(t_{i}, y_{i}))
\end{align}
$

```

#### Ordine di Convergenza (Heun)
```ad-Teo
title: Ordine di Convergenza

Il [[#Metodo di Heun]] ha **ordine di convergenza** pari a 2:
$
p = 2
$
```
### Metodo di Runge-Kutta

```ad-tip
title: Algoritmo Metodo di Runge-Kutta

$
y_{i+1} = y_{i} + \frac{h}{6} [K_{1}(t_{i}, y_{i}) + 2K_{2}(t_{i}, y_{i}) + 2K_{3}(t_{i}, y_{i})+K_{4}(t_{i}, y_{i})] \qquad 0\le i \le n
$
dove
$
\begin{align}
K_{1}(t_{i}, y_{i}) &= f(t_{i}, y_{i}) \\
K_{2}(t_{i}, y_{i}) &= f\left( t_{i}+ \frac{h}{2}, y_{i} + \frac{h}{2} K_{1}(t_{i}, y_{i})\right)\\
K_{3}(t_{i}, y_{i}) &= f\left( t_{i}+ \frac{h}{2}, y_{i} + \frac{h}{2}K_{2}(t_{i}, y_{i}) \right)\\
K_{4}(t_{i}, y_{i}) &= f(t_{i} + h,y_{i} + hK_{3}(t_{i}, y_{i}) )

\end{align}
$
```

#### Ordine di Convergenza (Runge-Kutta)
```ad-Teo
title: Ordine di Convergenza (RK)

Il [[#Metodo di Runge-Kutta]] ha **ordine di convergenza** pari a 4:
$
p = 4
$
```

___

## Metodi alle differenze finite
Metodi per la risoluzione di [[#Problema ai limiti]].
Vedremo in particolare solo problemi ai limiti **lineari**:
$$
\begin{cases}
y''(x) = p(x)y'(x) + q(x)y(x)-r(x) \\
y(a) = \alpha, y(b) = \beta
\end{cases}
$$

### Convergenza
#### Condizione Sufficiente di esistenza e unicità della soluzione
```ad-Teo
title: CS di esistenza della soluzione
Se
- $p(x), q(x), r(x) \in C[a,b]$
- $\max\limits_{x \in [a,b]}|p(x)| = k$
- $q(x) > 0 \qquad x \in [a,b]$

**Allora**
Esiste un'**unica** soluzione al [[#Problema ai limiti]]
```

### Algoritmo
#### Discretizzazione
Intervallo di discretizzazione $I = [a,b]$

- Nodi: $x_{i} = x_{0} + ih \qquad 0\le i \le N+1 \qquad$ con $N+2$ nodi
- Approssimazioni: $y_{i} \approx y(x_{i}) \qquad 1 \le i \le N$

Il passo è
$$
h = \frac{b-a}{N+1}
$$
Si ha che
$$
\begin{align}
y_{0} &= y(x_{0}) = y(a) = \alpha \\
y_{N+1} &= y(x_{N+1}) = y(b) = \beta
\end{align}
$$
#### Collocazione

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗

#### Schema numerico

```ad-tip
title: Aloritmo

$
\begin{cases}
-\frac{y_{i+1} -  2y_{i} + y_{i-1}}{h^{2}} + p(x_{i}) \frac{y_{i+1}-y_{i-1}}{2h} + q(x_{i})y_{i} = r(x_{i}) \\
y_{0} = \alpha, \quad y_{N+1} = \beta
\end{cases}
$

```


### Errori
$$
e_{i} = y(x_{i}) - y_{i} \qquad 0 \le 0 \le N+1
$$


```ad-note
title: Osservazione

$
e_{0} = e_{N+1} = 0
$
Perché in quei punti abbiamo le condizioni al contorno.

```


#### Errore di troncamento locale
```ad-Teo
title: Errore di troncamento Metodo alle differenze finite

$
\tau(x_{i}, y, h) = \frac{h^{2}}{12} (y'^{v}(\xi_{i}) - 2p(x_{i})y'''(\eta_i ))
$
```


#### Errore di troncamento globale
```ad-Teo
title: Maggiorazione Errore globale di troncamento

L'errore di troncamento globale può essere maggiorato come segue
$
\max_{0 \le i \le N+1} |e_{i}| \le M \max_{1 \le i \le N} |\tau(x_{i}, y, h)
$
dove $M = \frac{1}{L}$ con $L = \min_{[a,b]}q(x)$ (vd. [[#Condizione sufficiente di stabilità]]).

```


#### Consistenza differenze finite 
```ad-Definizione
title: Consistenza differenze finite 

$
\lim_{h \to 0} \tau(x_{i},y, h) = 0
$
è sempre vero per le $\tau$ come sopra

```

### Stabilità differenze finite 
```ad-Definizione
title: Stabilità differenze finite

Un [[#Metodi alle differenze finite]] è stabile se 
$
k = \max_{0 \le i \le i} |e_{i}| \le cost < \infty \qquad \forall h \text{ fissato}
$
$
L = \min_{[a,b]} q(x) > 0
$

```

La stabilità va studiata schema per schema.

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗

#### Condizione sufficiente di stabilità
```ad-Teo
title: CS di stabilità

Lo schema lineare è stabile se, siano verificate le seguenti:

$
\begin{align}
k &= \max_{[a,b]} |p(x)| < \infty \\
L &= \min_{[a,b]} q(x) > 0
\end{align}
$
è verificato che:

$
hK \le 2
$


```



❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗ 
❗❗❗❗❗❗❗❗❗❗❗❗❗

	Negli appunti c'è scritto anche se valgono i) e ii). A che si riferisce?


### Convergenza differenze finite

#### Condizioni di convergenza differenze finite
```ad-Teo
title: Condizione CN/CS/CNS di Convergenza
```

##### Teorema di Lax
```ad-Teo
title: Teo di Lax

Per uno schema alle differenze finite (qualunque), la convergenza è equivalente alla [[#Consistenza differenze finite]] + [[#Stabilità differenze finite]].

```


#### Ordine di convergenza differenze finite

```ad-Teo
title: Ordine di Convergenza

Il metodo di ??? ha [[01. Introduzione ai metodi numerici#Ordine di convergenza|ordine di convergenza]] pari a:
$
p = 
$

___
**Dimostrazione:**

```


### Stima iterazioni necessarie differenze finite
Se rilevante

## Criterio di arresto differenze finite
Se rilevante
### Criterio di arresto a posteriori differenze finite

Posso interrompere l'algoritmo quando l'errore al passo k è minore di una tolleranza scelta $\varepsilon$.
$$
|e_{k}| \le \varepsilon
$$

## Implementazione in Matlab differenze finite

Riportare la function per l'applicazione del metodo in matlab

```matlab


```





