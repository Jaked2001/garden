---
{"dg-publish":true,"permalink":"/universita/triennale/2-anno/2-semestre/analisi-numerica/appunti/03-2-il-metodo-delle-tangenti/"}
---

# 3.2 Il metodo delle Tangenti

___

Il **metodo delle tangenti**, detto anche **metodo di Newton** sfrutta il [[Polinomio di Taylor\|Polinomio di Taylor]] per approssimare il valore delle funzioni. In questo caso è necessario scegliere un'approssimazione iniziale $x_{0}$.

Si tratta di fatto di un metodo di linearizzazione. Infatti, come vedremo, interromperemo il Polinomio di Taylor al primo ordine, confondendo quindi il valore della funzione con il valore della tangente.

![Metodo_delle_tangenti.png](/img/user/Universit%C3%A0/Triennale/2%C2%B0%20anno/2%C2%B0%20Semestre/Analisi%20Numerica/allegati/Metodo_delle_tangenti.png)

## ☑️ Ipotesi

Per l'applicabilità del metodo di newton, la funzione deve rispettare le seguenti ipotesi:

```ad-tip
title: Ipotesi 
1. $I$ è [[01. Introduzione ai metodi numerici#Intervallo di separazione|Intervallo di separazione]]
1. $f(x) \in \mathrm{C^{1}}(I = [a,b])$
2. $f'(x) \ne 0 \, \forall k$

```

## Algoritmo

Scelgo un'approssimazione iniziale $x_{0}$.

Approssimo la funzione $f(x)$ con un polinomio di Taylor al secondo ordine:
$$
f(x) = f(x_{0}) + f'(x_{0})(x-x_{0}) + \frac{1}{2}f''(\tau)(x-x_{0})^{2} \text{ con } \tau \in [x_{0}, x]
$$
Se cerco la radice di $f(x)$ impongo la funzione approssimata uguale a 0:
$$
f(x) = f(x_{0}) + f'(x_{0})(x-x_{0}) \stackrel{!}{=} 0
$$
da cui
$$
x_{1} = x_{0} - \frac{f(x_{0})}{f'(x_{0})}
$$
motivo per cui dobbiamo imporre anche $f'(x_{k}) \ne 0$ tra le [[#☑️ Ipotesi]].

Dopodiché approssimo nuovamente la funzione con Taylor usando stavolta $x_{1}$ al posto di $x_{0}$:
$$
f(x) = f(x_{1}) + f'(x_{1})(x-x_{1}) \stackrel{!}{=} 0
$$
da cui
$$
x_{2} = x_{1} - \frac{f(x_{1})}{f'(x_{1})}
$$
Continuo ad iterare questo procedimento. Avrò così, **in definitiva**:
$$
x_{k} = x_{k-1} - \frac{f(x_{k-1})}{f'(x_{k-1})}
$$
Il metodo [[#Convergenza|convergerà]] per alcuni valori di $x_{0} \in J \subset I$ (Ossia scegliendo $x_{0}$ *abbastanza vicino* a $\xi$).

```ad-important
title: Algoritmo

$
\begin{cases}
x_{0} \qquad \text{Punto iniziale}\\
x_{k} = x_{k-1} - \frac{f(x_{k-1})}{f'(x_{k-1})}
\end{cases}
$
```

## Convergenza

```ad-Teo
title: Condizione Necessaria di Convergenza
Sia $I = [a,b]$ un intervallo di separazione di uno zero di $f$;

Sia:

- $f \in C^{2}[a,b]$
- $f'(x) \ne 0$
- $x \in [a,b]$

**Allora** esiste un intorno $J$ di $\xi$, con $J \subset I$, tale che, se $x_{0} \in J$, la successione risulta convergente a $\xi$.

Inoltre, se $f\in C^{3}[a,b]$ l'[[Università/Triennale/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Ordine di convergenza\|ordine di convergenza]] è $\ge 2$.
```

___

```ad-Teo
title: Convergenza con Estremo di Fourier

Sia:

- $f \in C^{2}[a,b]$
- $f(a) \cdot f(b) < 0$
- $f''(x) \ne 0$

per $x \in [a,b]$, sia $x_{0}$ l'[[#Estremo di Fourier]] di $[a,b]$, **allora:**

1. Esiste un unico $\xi \in (a,b)$, tale che $f(\xi) = 0$
2. La successione $\{ \varphi(x_{n}) \}$ è monotona e converge a $\xi$
3. Se $f \in C^{3}[a,b]$, la convergenza è quadratica


```



### Estremo di Fourier

Se $f''(x) \ne 0 \,\,\,\,\, \forall x \in I$ si può dimostrare che si può scegliere uno dei due estremi di $I$ come approssimazione iniziale $x_{0}$.

Supponiamo di avere una funzione **monotona**, **senza massimi o minimi locali**, a **concavità fissa**.
L'**estremo di Fourier** di un intervallo è quello dei due tale che il prodotto $f(a)\cdot f''(a) > 0$ dove $a$ è l'estremo.


### Ordine di convergenza

```ad-Teo
title: Ordine di Convergenza per Tangenti
$p = 2$
```


Per calcolare l'[[01. Introduzione ai metodi numerici#Ordine di convergenza|ordine di convergenza]] devo calcolare il limite:
$$
\lim_{k \to \infty} \frac{|e_{k+1}|}{|e_{k}|^{p}} = \text{C} > 0 \,\,\,\,\,\, p \ge 1
$$
Calcolo $|e_{k+1}|$:

$$
|e_{k+1}| = \xi - x_{k+1} = \left( \xi - \frac{f(\xi)}{f'(\xi)} \right) - x_{k+1}
$$
essendo  $\frac{f(\xi)}{f'(\xi)} = 0$ perché $f(\xi) = 0$.

$$
\begin{align}
|e_{k+1}| = \xi - x_{k+1} &= \left( \xi - \frac{f(\xi)}{f'(\xi)} \right) - x_{k+1} = \\

&= \left( \xi - \frac{f(\xi)}{f'(\xi)} \right) - \left( x_{k} - \frac{f(x_{k})}{f'(x_{k})} \right)  = \\

&= (\xi - x_{k}) - \left( \frac{f(\xi)}{f'(\xi)} - \frac{f(x_{k})}{f'(x_{k})} \right) = \\

\end{align}
$$
Sviluppando $f(x_{k})$ e $f'(x_{k})$ in serie di Taylor e ricordando $\xi - x_{k} = e_{k}$ ottengo:

$$
\begin{align}

|e_{k+1}| &= \left| 
e_{k} - \left( \frac{f(\xi)}{f'(\xi)} - \frac{f(\xi) + f'(\xi)(-e_{k}) + \frac{1}{2} f''(\xi)e_{k}^{2} }{f'(\xi)} \right)
\right| \\

&= \frac{1}{2} \left| \frac{f''(\xi)}{f'(\xi)} \right| |e_{k}^{2}|
\end{align}
$$

Si ha quindi che 

$$
\frac{|e_{k+1}|}{|e_{k}|^{p}} \stackrel{p = 2}{\approx} \frac{1}{2} \left| \frac{f''(\xi)}{f'(\xi)} \right|
$$
quindi

$$
\lim_{k \to \infty} \frac{|e_{k+1}|}{|e_{k}|^{2}} = \frac{1}{2} \left| \frac{f''(\xi)}{f'(\xi)} \right|
$$
Allora **l'ordine di convergenza** del metodo delle tangenti è $p = 2$.

## Implementazione in Matlab

```matlab
function xk = tangenti(x0,f, df, N_max)

% Input:
%   x0: approx iniziale
%   f, df: funzione e derivata
%   N_max: numero di iterazioni
% 
% Output:
%   xk: approx dello zero

for i = 1:N_max
    xk = x0 - f(x0) / df(x0);
    errk = abs(xk - x0);
    x0 = xk;
    fprintf('k = %3i x_k = %14.12f err=%9.3e f=%18.12f f''=%18.12f \n', i, xk, errk, f(x0), df(x0));
end

end
```
# Metodo di Newton in $\mathbb{R}^n$

Al link [[Università/Triennale/2° anno/2° Semestre/Analisi Numerica/Appunti/03.4 Metodo di Newton in n dimensioni\|03.4 Metodo di Newton in n dimensioni]]

Se in $\mathbb{R}$ possiamo riscrivere una funzione sviluppandola in un Polinomio di Taylor, lo stesso possiamo fare con $\mathbb{R}^{n}$.

Sia $F(X): \mathbb{R}^{n} \to \mathbb{R}^{n}$, posso scrivere
$$
F(X) = F(X^{(k)}) + J_{F}(X^{(k)})(X-X^{(k)}) + ...
$$

dove 
$$
\boldsymbol J_F(\boldsymbol X) = 
\begin{bmatrix} 

\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots &\frac{\partial f_1}{\partial x_n}  \\ 

\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots &\frac{\partial f_2}{\partial x_n}  \\ 

\vdots & \vdots & \ddots & \vdots \\ 

\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots &\frac{\partial f_n}{\partial x_n}  \\ 

\end{bmatrix}
$$
