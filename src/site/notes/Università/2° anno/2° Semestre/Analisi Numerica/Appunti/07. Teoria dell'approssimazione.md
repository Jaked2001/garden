---
{"dg-publish":true,"permalink":"/universita/2-anno/2-semestre/analisi-numerica/appunti/07-teoria-dell-approssimazione/"}
---


# 7. Teoria dell'approssimazione
La teoria dell'approssimazione è una branchia dell'analisi numerica utile ad approssimare una serie di dati con una funzione di qualche tipo. 
Può essere divisa in due macroaree:
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#Approssimazione\|#Approssimazione]]
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#Interpolazione\|#Interpolazione]]

```ad-example
title: Data Fitting

Si hanno delle misurazioni sperimentali dell'allungamento di una molla:

| $x$ | 2   | 3   | 4   | ... | 10  | 
| --- | --- | --- | --- | --- | --- |
| $F$ | 7.0 | 8.3 | 9.4 | ... | 15.9    |

Legge di Hooke:
$$
F(x) = -k(x-l) = -kx + kl
$$

```chart
type: line
labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
series:
  - title: x
    data: [3, 4.5, 7.0,8.3, 9.4, 10.2, 11.4 ,12.8, 14.0, 14.2,15.9 ]
tension: 0.2
width: 80%
labelColors: false
fill: false
beginAtZero: false
bestFit: true
bestFitTitle: undefined
bestFitNumber: 0
```

```ad-example
title: Interpolazione

Braccio robotico deve fare dei fori.
Dobbiamo comunicare: 
- Posizione dei fori
- Percorso più breve
Uso polinomi a tratti di grado 3 (Spline)
```

## Approssimazione ai minimi quadrati
Sia $\varphi (x)$ la funzione approssimante
Vogliamo minimizzare la distanza tra i dati e la funzione approssimante. 

Calcolo la distanza come norma Euclidea: **scarto quadratico**
$$
\sum\limits_{i=0}^{n} (\varphi (x_{i}) - y_{i})^{2}
$$
Voglio minimizzare lo scarto quadratico

```ad-note
title: Osservazione
Posso anche pesare i dati:
$$
\sum\limits_{i=0}^{n} w_{i}(\varphi (x_{i})- y_{i})^{2}
$$

```

### Possibili funzioni approssimanti
#### Retta
```ad-Teo
title:
$$
p_{1}(x) = ax+b
$$
2 parametri incogniti: $a,b$

```

#### Polinomi algebrici

Per fenomeni regolari dove non prevediamo grosse variazioni

```ad-Teo
title:
$$
p_{m}(x) = a_{0} + a_{1}x + a_{2}x^{2} + \cdots + a_{m}x^{m} 
$$
$m+1$ parametri incogniti
```

#### Polinomi trigonometrici
```ad-Teo
title: 

$$
T_{M} (x)= \frac{a_{0}}{2} + \sum\limits_{k+1}^{M}(a_{k} \cos(kx) + b_{k} \sin(kx)) 
$$
$2m+1$ parametri incogniti

```

#### Funzioni lineari logaritmiche

#### Funzioni esponenziali


### Problema fondamentale
Dato il vettore $\Gamma$:
$$
\Gamma = \begin{bmatrix}
\gamma_{0} \\
\vdots \\
\gamma_{M}
\end{bmatrix}
$$
di $M+1$ parametri.
- nodi: $x_{0}<x_{1}< \cdots <x_{n}$

Lo [[#Scarto quadratico]] è 
$$
\sigma^{2} = \sum\limits_{i=0}^{n}(\varphi (x_{i}) - y_{i}) = \sum\limits_{i=0}^{n} ( (\gamma_{0}\psi_{0}(x_{i}) + \gamma_{1}\psi_{1}(x_{i}) + \cdots + \gamma_{M}\psi_{M}(x_{i})) - y_{i} )^{2}
$$
Le incognite sono i parametri $\gamma$.

L'obiettivo è minimizzare rispetto ai parametri la funzione:
$$
\sigma^{2} = F(\gamma_{0}, \gamma_{1}, \cdots ,\gamma_{M})
$$
Procedo al calcolo della derivata:
$$
\frac{\partial F}{\partial \gamma_{k}}(\gamma_{0}, \gamma_{1}, \cdots, \gamma_{M} ) \stackrel{!}{=} 0 \qquad 0 \le k \le M
$$
$$
\frac{\partial F}{\partial \gamma_{k}}  = 
\begin{cases}
\frac{\partial F}{\partial \gamma_{0}} = \sum\limits_{i=0}^{n}(2(\gamma_{0}\psi_{0}(x_{i}) + \cdots + \gamma_{M}\psi_{M}(x_{i}) - y_{i}) \cdot \psi_{0}(x_{i})) &\stackrel ! = 0 \\
\frac{\partial F}{\partial \gamma_{1}}  = \sum\limits_{i=0}^{n}(2(\gamma_{0}\psi_{0}(x_{i}) + \cdots + \gamma_{M}\psi_{M}(x_{i}) - y_{i}) \cdot \psi_{1}(x_{i})) &\stackrel ! = 0 \\
\vdots \\
\frac{\partial F}{\partial \gamma_{M}}  = \sum\limits_{i=0}^{n}(2(\gamma_{0}\psi_{0}(x_{i}) + \cdots + \gamma_{M}\psi_{M}(x_{i}) - y_{i}) \cdot \psi_{M}(x_{i})) &\stackrel ! = 0
\end{cases}
$$

Distribuendo la sommatoria, ottengo
$$
\frac{\partial F}{\partial \gamma_{k}}  = \gamma_{0} \underbrace{\sum\limits_{i=0}^{n} \psi_{0}(x_{i})\psi_{k}(x_{i})}_{h_{k1}}  + \cdots + \gamma_{M}\sum\limits_{i=0}^{n} \psi_{M} (x_{i})\psi_{k}(x_{i}) - \sum\limits_{i=0}^{n} y_{i}\psi_{k}(x_{i}) = 0
$$
da cui definisco una matrice $H$, simmetrica:
$$
H = \begin{bmatrix}
h_{00} & h_{01} & \cdots & h_{0M} \\
h_{10} & h_{11} & \cdots & h_{1M} \\
\vdots & \vdots & \ddots & \vdots \\
h_{M0} & h_{M1} & \cdots & h_{MM}  
\end{bmatrix}
\in \mathbb{R}^{(M+1) \times (M+1)}
$$
le cui entrate sono:
$$
h_{ij} = \sum\limits_{i=0}^{n} \psi_{i}(x_{i})\psi_{j}(x_{i})
$$
Posso quindi costruire la matrice $V$ di seguito e porre $H = V^{T}V$:
$$
V = \begin{bmatrix}
\psi_{0}(x_{0}) & \psi_{1}(x_{0}) & \cdots & \psi_{M}(x_{0}) \\
\psi_{0}(x_{1}) & \psi_{1}(x_{1}) & \cdots & \psi_{M}(x_{1}) \\
\vdots & \vdots & \ddots & \vdots \\
\psi_{0}(x_{n}) & \psi_{1}(x_{n}) & \cdots & \psi_{M}(x_{n}) \\
\end{bmatrix}
\in \mathbb{R}^{(n+1)\times(M+1)}
$$
- $H$ è [[02. Richiami di algebra lineare#Matrice definita positiva|definita positiva]]
- $H$ è regolare
- Esiste **una sola** soluzione

Calcolo l'Hessiana
$$
H_{\text{ess}} = \left(\frac{\partial^{2}F}{\partial \gamma_{j} \partial \gamma_{k}} \right)_{0 \le j, k\le M} = 2H
$$
Rispetto alla derivata prima, i $\Gamma$ tali che la derivata prima fa $0$ sono punti di minimo se:
- $\det H_{\text{ess}} > 0$
- $H_{\text{ess}}$ è definita positiva

#### Polinomio Algebrico ai minimi quadrati
- Dati: $\{ x_{i}, y_{i} \}\qquad 0 \le i \le n$

definisco il polinomio:
$$
\begin{align}
\mathcal{P}_{M} &= a_{0} + a_{1}x + \cdots + a_{M}x^{M} \\
&= \gamma_{0}\psi_{0}(x) + \gamma_{1}\psi_{1}(x) + \cdots + \gamma_{M}\psi_{M}(x)
\end{align}
$$

si ha quindi che $\psi_{k}(x) = x^{k}$

gli elementi della matrice $H$ sono:
$$
h_{jk} = \sum\limits_{i=0}^{n}\psi_{j}(x_{i})\psi_{k}(x_{i}) = \sum\limits_{i=0}^{n}x_{i}^{j}x_{i}^{k} = \sum\limits_{i=0}^{n}x_{i}^{j+k} \stackrel{\triangle}{=} s_{j+k}
$$
Termine noto:
$$
b_{k} = \sum\limits_{i=0}^{n}y_{i}\psi_{k}(x_{i}) =  \sum\limits_{i=0}^{n}y_{i}x_{i}^{k}
$$

##### Retta di regressione M = 1
[[#Polinomio Algebrico ai minimi quadrati]] di grado 1
$$
r(x) = a_{0}+a_{1}x
$$
si ha 
$$
\begin{align}
H \Gamma &= B \\
\begin{bmatrix}
h_{00} & h_{01} \\
h_{10} & h_{11}
\end{bmatrix}
\begin{bmatrix}
a_{0} \\
a_{1}
\end{bmatrix} &= 
\begin{bmatrix}
b_{0} \\
b_{1}
\end{bmatrix} \\
\begin{bmatrix}
s_{0} & s_{1} \\
s_{1} & s_{2}
\end{bmatrix}
\begin{bmatrix}
a_{0} \\
a_{1}
\end{bmatrix} &= 
\begin{bmatrix}
b_{0} \\
b_{1}
\end{bmatrix}
\end{align}
$$

dove
$$
\begin{align}
a_{0} &= \frac{b_{0}s_{2}-b_{1}s_{1}}{s_{0}s_{2}-s_{1}^{2}} \\
a_{1} &= \frac{b_{1}s_{0}-b_{0}s_{1}}{s_{0}s_{2}-s_{1}^{2}}
\end{align}
$$

```ad-tip
title: Metodo
Retta di regressione:
$$
r(x) = a_{0}+a_{1}x
$$
Elementi matrice $H$:
$$
h_{jk} = \sum\limits_{i=0}^{n}\psi_{j}(x_{i})\psi_{k}(x_{i}) = \sum\limits_{i=0}^{n}x_{i}^{j}x_{i}^{k} = \sum\limits_{i=0}^{n}x_{i}^{j+k} \stackrel{\triangle}{=} s_{j+k}
$$
Termine noto $B$:
$$
b_{k} = \sum\limits_{i=0}^{n}y_{i}\psi_{k}(x_{i}) =  \sum\limits_{i=0}^{n}y_{i}x_{i}^{k}
$$
Coefficienti:
$$
\begin{align}
a_{0} &= \frac{b_{0}s_{2}-b_{1}s_{1}}{s_{0}s_{2}-s_{1}^{2}} \\
a_{1} &= \frac{b_{1}s_{0}-b_{0}s_{1}}{s_{0}s_{2}-s_{1}^{2}}
\end{align}
$$
```




```ad-example

Costruire la retta di regressione per le seguenti misurazioni:

| $i$     | 0   | 1   | 2   | 3    | 4    | 5    | 6    |
| ------- | --- | --- | --- | ---- | ---- | ---- | ---- |
| $x_{i}$ | 2   | 3   | 4   | 5    | 6    | 8    | 10   |
| $F_{i}$ | 7.0 | 8.3 | 9.4 | 11.3 | 12.3 | 14.4 | 15.9 |

$$
F(x) = k(x-l) = kx - kl
$$
In questo caso $a_{1} = k$
$n = 7$
Calcolo
$$
\begin{align}
s_{0} &= \sum\limits_{i=0}^{n} x_{i}^{0} = 7 \\
s_{1} &= \sum\limits_{i=0}^{n} x_{i}^{1+0} = 2+3+4+5+6+8+10 = 38 \\
s_{2} &= \sum\limits_{i=0}^{n} x_{i}^{2+0} = 4+9+16+25+36+64+100 = 254
\end{align}
$$
per cui:
$$
H = \begin{bmatrix}
7 & 38 \\
38 & 254
\end{bmatrix}
$$
Calcolo il termine noto:
$$
\begin{align}
b_{0} &= \sum\limits_{i=0}^{n}y_{i}x_{i}^{0} = 7.0 \cdot 2^{0} + \cdots
 + 15.9 = 78.60 \\
b_{1} &= \sum\limits_{i=0}^{n}y_{i}x_{i}^{1} = 7.0 \cdot 2 + \cdots
 + 15.9 \cdot 10 = 481
\end{align}
$$

Calcolo i parametri
$$
\begin{align}
a_{0} &= \frac{78.6 \cdot 7-481 \cdot 38}{7\cdot 38-38^{2}} &\approx 5.049 = -kl  \\
a_{1} &= \qquad \qquad \cdots &\approx 1.1383 = k
\end{align}
$$
```chart
type: line
labels: [2,3,4,5,6,8,10]
series:
  - title: Forza
    data: [7.0,8.3,9.4,11.3,12.3,14.4,15.9]
tension: 0.2
width: 80%
labelColors: false
fill: false
beginAtZero: false
bestFit: false
bestFitTitle: Retta di regressione
bestFitNumber: 0
```


### Approssimazione trigonometrica
Ha periodo $T = 2 \pi$
- Nodi: $\{ x_{i}, y_{i} \} \qquad 0 \le i \le n$
- Passo: $\frac{2\pi}{n+1}$

La funzione approssimante è una funzione del tipo:
$$
T_{M} (x)= \frac{a_{0}}{2} + \sum\limits_{k+1}^{M}(a_{k} \cos(kx) + b_{k} \sin(kx)) 
$$
Dobbiamo determinare i coefficienti $a_{k}, b_{k}$: $2M+1$ coefficienti incogniti.

```ad-tip
title: Metodo
Funzione approssimante:
$$
T_{M} (x)= \frac{a_{0}}{2} + \sum\limits_{k+1}^{M}(a_{k} \cos(kx) + b_{k} \sin(kx))
$$

Coefficienti:
$$
\begin{align}
a_{0} &= \frac{2}{n+1} \sum\limits_{i=0}^{n} y_{i} \\
a_{k} &= \frac{2}{n+1} \sum\limits_{i=0}^{n} y_{i} \cos (kx_{i}) \quad 1 \le k \le M \\
b_{k} &= \frac{2}{n+1} \sum\limits_{i=0}^{n} y_{i} \sin (kx_{i}) \quad 1 \le k \le M
\end{align}
$$

```




## Interpolazione

```ad-Definizione
title: Interpolazione

Un tipo di approssimazione tale che la funzione approssimante $f_{n}$ rispetta la proprietà:
$$
f_{n}(x_{i}) = f_{i}
$$
dove $f_{i}$ sono i valori disponibili ed assunti da una funzione $f$ nei nodi $x_{i}$

```

### Interpolazione polinomiale
Siano assegnati i valori $\{f_{i}\}$ che una funzione $f$, di una variabile reale $x$, assume in $n+1$ nodi distinti $\{x_{i}\}, \quad i =.0,1,...,n$.
Il problema dell'**interpolazione polinomiale** consiste nella determinazione di un polinomio di grado minimo che passi per i punti $P_{i}(x_{i}, f_{i}), \quad i = 0,1,...,n$. Essondo i punti $n+1$, è sufficiente un polinomio di grado $n$: $p_{n} \in \mathbb{P}_{n}$ che risulti:
$$
p_{n}(x_{i}) = f_{i} \qquad i = 0,1,...,n
$$
Si ha che il generico polinomio di grado $n$ è
$$
p_{n} = a_{0} + a_{1}x + a_{2}x^{2} + \cdots + a_{n}x^{n}
$$
Per la determinazione dei coefficienti ci si riduce al sistema:
$$
\begin{cases}
a_{0} + a_{1}x_{0} + a_{2}x^{2}_{0} + \cdots + a_{n}x^{n}_{0} &= f_{0} \\
a_{0} + a_{1}x_{1} + a_{2}x^{2}_{1} + \cdots + a_{n}x^{n}_{1} &= f_{1} \\
\vdots \\
a_{0} + a_{1}x_{n} + a_{2}x^{2}_{n} + \cdots + a_{n}x^{n}_{n} &= f_{n}
\end{cases}
$$
dal quale otteniamo la matrice dei coefficienti (di questo sistema), detta <mark style="background: #ADCCFFA6;">matrice di Vandermonde:</mark>
$$
V = \begin{bmatrix}
1 & x_{0} & x_{0}^{2} & \cdots & x_{0}^{n} \\
1 & x_{1} &x_{1}^{2} & \cdots & x_{1}^{n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n} & x_{n}^{2} & \cdots & x_{n}^{n}
\end{bmatrix}
$$
Questa matrice, nella base dei monomi, può risultare [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Condizionamento\|mal condizionata]]. Si preferisce quindi usare una forma diversa del polinomio, i [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#Polinomi di base di Lagrange\|#Polinomi di base di Lagrange]].

#### Polinomi di base di Lagrange
Si tratta di $n+1$ polinomi di grado $n$ che chiameremo $\{l_{i}(x)\}$:
$$
l_{0}(x), l_{1}(x), ..., l_{n}(x) \in \mathbb{P}_{n}
$$
ognuno dei quali soddisfa la seguente proprietà:
$$
l_{k}(x_{i}) = \delta_{ki} = \begin{cases}
1,& \quad i=k \\
0,& \quad i \ne k \\
\end{cases}
$$
Per cui:
$$
l_{k}(x) = \prod_{i = 0, i \ne k}^{n} \frac{x-x_{i}}{x_{k}-x_{i}}
$$
Si può a questo punto scrivere il polinomio interpolatore come:
$$
p_{n}(x) = f_{0}l_{0}(x) + f_{1}l_{1}(x)+ \cdots + f_{n}l_{n}(x)
$$

In breve:

```ad-tip
title: Metodo
Costruisco i polinomi di Lagrange:
$$
l_{k}(x) = \prod_{i = 0, i \ne k}^{n} \frac{x-x_{i}}{x_{k}-x_{i}}
$$
Scrivo il polinomio interpolatore come:
$$
p_{n}(x) = f_{0}l_{0}(x) + f_{1}l_{1}(x)+ \cdots + f_{n}l_{n}(x)
$$
```

```ad-example
title: Esempio

❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗
```

##### Errore di interpolazione
Nell'interpolazione ci sono 2 sorgenti di errore:
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#Errore di troncamento (interpolazione)\|#Errore di troncamento (interpolazione)]]: quello dovuto alla sostituzione della funzione con un polinomio
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#Errore di propagazione (interpolazione)\|#Errore di propagazione (interpolazione)]]: quello dovuto a un errore nei dati

Definito $p_{n}^{\star}$ il polinomio interpolatore ideale calcolato in corrispondenza ai valori esenti da errori.
$$
p_{n}^{\star} = f(x_{0})l_{0}(x) + f(x_{1})l_{1}(x)+ \cdots +f(x_{n})l_{n}(x)
$$
e $p_{n}$ il polinomio interpolatore trovato:
$$
p_{n}(x) = f_{0}l_{0}(x) + f_{1}l_{1}(x)+ \cdots + f_{n}l_{n}(x)
$$
definiamo l'errore di interpolazione come segue:

```ad-Definizione
title: Errore di interpolazione ($E_{int}(x)$)

$$
\begin{align}
E_{int}(x) &= f_{x} - p_{n}(x) =  \\
&= \underbrace{f(x) - p_{n}^{\star}(x)}_{E_{n}(x)}  + \underbrace{p_{n}^{\star}(x) - p_{n}(x)}_{E_{n}^{\star}(x)} \\
&= E_{n}(x) + E_{n}^{\star}(x)
\end{align}
$$
dove
- $E_{n}(x) =$ Errore di troncamento
- $E_{n}^{\star}(x) =$ Errore di propagazione

```

Per cui è ovvio che, dalla [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/07. Teoria dell'approssimazione#\|##disuguaglianza triangolare]]
```ad-Teo
title: Maggiorazione Errore totale
$$
|E^{TOT}_{n}(x)| \le |E_{n}^{\star}(x)| + |E_{n}(x)|
$$

```

###### Errore di troncamento (interpolazione)
$$
E_{n} (x) = \Pi_{n}(x) \cdot \frac{f^{(n+1)}(\xi(x))}{(n+1)!} \qquad \xi(x) \in [x_{0}, x_{}]
$$
con $\Pi_{n}(x)$ polinomio nodale
$$
\Pi_{n}(x) = \prod_{i=0}^{n}(x-x_{i})
$$
Dell'errore di troncamento, possiamo dare la seguente maggiorazione:
```ad-Teo
title: Errore di propagazione

$$
|E_{n}(x)| \le |\Pi_{n}(x)| \max_{x_{0}, x_{n}} \frac{|f^{(n+1)}(x)|}{(n+1)!}
$$

```


###### Errore di propagazione (interpolazione)
```ad-Teo
title: Errore di propagazione


$$
|E_{n}^{\star}(x)|  \le \varepsilon \sum\limits_{i= 0}^{n} |l_{i}(x)| = \varepsilon \Lambda(x)
$$
dove
- $\varepsilon =$ l'errore sui dati
- $l_{i}(x) =$ è l'$i$-esimo polinomio in base di Lagrange
- $\Lambda(x)=$ è la funzione di Lebesgue
ricordo:
$$
l_{i}(x) = \prod_{\begin{align}
j = 0 \\
i \ne j
\end{align}}^{n}
\frac{x-x_{j}}{x_{i}-x_{j}}
$$


```


### Interpolazione trigonometrica
```ad-tip
title: Metodo

- Nodi: $\{x_{i}, y_{i}\} \qquad 0 \le i \le n$
  $x_{i} = i \frac{2 \pi}{n+1}$

**Polinomio interpolatore trigonometrico**
$$
T_{n}(x) = \frac{a_{0}}{2} + \sum\limits_{k=0}^{\frac{n}{2}}(a_{k}\cos(kx) + b_{k}\sin(kx))
$$
Coefficienti:
$$
\begin{align}
a_{k} = \frac{2}{n+1} \sum\limits_{i=0}^{n}y_{i} \cos(kx_{i}) \quad 0 \le k \le \frac{n}{2} \\
b_{k} = \frac{2}{n+1} \sum\limits_{i=0}^{n}y_{i} \sin(kx_{i}) \quad 1 \le k \le \frac{n}{2}
\end{align}
$$
```


### Interpolazione Spline
#### Partizione
```ad-Definizione
title: Partizione ($\Delta$)

La **partizione** $\Delta$ di un intervallo $[a,b]$ è un insieme di punti tali che
$$
\Delta: \qquad a = x_{0} < x_{1} < \cdots < x_{n-1} < x_{n} = b
$$

```

#### Funzione Spline
```ad-Definizione
title: Funzione Spline

La **funzione Spline** di grado $m$ associata alla partizione $\Delta$ è $S_{m}(x)$ dove
1. $S_{m}(x)$ è un polinomio di grado $m$ nei sotto-intervalli $[x_{i-1},x_{i}], \quad 1 \le i \le n$
2. $S_{m}(x) \in C^{m-1} [a,b] \iff S_{m}^{(k)}(x_{i}^{-}) = S_{m}^{(k)}(x_{i}^{+}) \qquad \forall 1 \le k \le m-1$

```

#### Spline lineare
$S_{1}(x)$ è un polinomio di grado 1 nei sottointervalli $T_{i} = [x_{i-1}, x_{i}] \quad 1 \le i \le n$ tale che
$$
S_{1}(x_{i}^{-}) =S_{1}(x_{i}^{+})  
$$
- Nodi: $\{x_{i}, y_{i}\} \quad 0 \le i \le n$
La spline lineare interpolante è un polinomio a tratti continuo che soddisfa le condizioni di interpolazione
$$
S_{1}(x_{i}) = y_{i}
$$

![Schermata 2023-07-03 alle 18.46.54.png](/img/user/Universit%C3%A0/2%C2%B0%20anno/2%C2%B0%20Semestre/Analisi%20Numerica/Appunti/allegati/Schermata%202023-07-03%20alle%2018.46.54.png)

```ad-tip
title: Metodo
Spline lineare interpolante, $x \in [x_{i-1}, x_{i}]$
$$
S_{1}(x) \frac{1}{h_{i}}((x_{i}-x)y_{i-1} + (x-x_{i-1})y_{i})
$$
___
**Base lagrangiana**:
$$
B_{l}(x) = \begin{cases}
\frac{1}{h_{l}} (x-x_{l-1}), \quad x_{l-1} \le x \le x_{l} \\
\frac{1}{h_{l}} (x_{l+1}-x_{l}), \quad x_{l} \le x \le x_{l+1} \\
0,  \qquad \text{Altrove}
\end{cases}
$$
Per cui la **spline lineare interpolante in base lagrangiana** diventa:
$$
S_{1}(x) = \sum\limits_{i=0}^{n}y_{i}B_{i}(x)
$$
```

