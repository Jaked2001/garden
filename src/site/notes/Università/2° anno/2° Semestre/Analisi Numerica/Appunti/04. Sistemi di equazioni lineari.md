---
{"dg-publish":true,"permalink":"/universita/2-anno/2-semestre/analisi-numerica/appunti/04-sistemi-di-equazioni-lineari/"}
---

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Matrice di Hilb\|#Matrice di Hilb]]
- Fattorizzazione di LU
# 4. Sistemi di equazioni lineari
___


È dato un sistema lineare $n\times n$, nella forma 
$$AX = B$$
dove:
- $A \in \mathbb{R}^{n\times n}$: Matrice dei coefficienti
- $X \in \mathbb{R}^{n}$: Vettore **incognito**
- $B \in \mathbb{R}^{n}$: Vettore dei **termini noti**

Vogliamo risolvere il problema cercando la soluzione $X$ che soddisfi l'equazione.

I metodi per la soluzione di sistemi lineari sono di tue tipi:
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Metodi diretti\|#Metodi diretti]]
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Metodi iterativi\|#Metodi iterativi]]


# Metodi diretti
```ad-Definizione
title: Metodi diretti

Metodi per la soluzione di sistemi lineari basati sulla trasformazione del sistema di partenza in uno equivalente che abbia una struttura particolarmente semplice.
- Sfruttano un numero finito di passi
- Teoricamente, i metodi diretti fornirebbero la soluzione esatta, se non fosse per gli errori di arrotondamento nei calcoli e errori di rappresentazione dei dati.

- Vengono usati quando la dimensione della matrice dei coefficienti non è troppo elevata.
```

Sono:
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Metodo di Cramer\|#Metodo di Cramer]]
- [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Metodo di eliminazione di Gauss\|#Metodo di eliminazione di Gauss]]

## Metodo di Cramer
### Algoritmo per Cramer
Dato un vettore delle incognite
$$
X =
\begin{bmatrix}
x_{1} \\ x_{2} \\ \vdots \\ x_{n}
\end{bmatrix}
$$
```ad-important
title: Algoritmo

Per il Teo di Kramer, $x_{i} = \frac{D_{i}}{\text{det}(A)}$, dove
$$
D_{i} = \mathrm{det}
\begin{bmatrix}
\mid & \mid  &  & \mid  & & \mid \\ 
A_{1} & A_{2} & \dots & B & \dots & A_{n} \\ 
\mid & \mid  &  & \mid  & & \mid \\ 
\end{bmatrix}
$$
Con $B$ nella $i$-esima colonna.
```

### Costo computazionale per Cramer
Poiché dobbiamo implementare questi metodi al calcolatore, ci chiediamo quale sia il [[Costo Computazionale\|Costo Computazionale]] ($C_{c}$) del Metodo di Cramer. Sappiamo che ad interessare sono solo le **moltiplicazioni** e le **divisioni**, mentre possiamo ignorare il costo computazionale di addizioni e sottrazioni.

Per calcolare un determinante, il costo computazionale è dato dal numero di moltiplicazioni.

Per ogni determinante, il numero di prodotti ($a_{1}\cdot a_{2} \dots a_{n}$) è $n!$.
Ogni prodotto contiene $n-1$ moltiplicazioni.
**Allora**, per un determinante si compiono $n!(n-1)$ moltiplicazioni.

Nel metodo di Cramer calcoliamo $n$ determinanti per le matrici $D_{i}$ e $1$ determinante per la matrice $A$.
Calcoliamo quindi $n+1$ determinanti.

In definitiva, il numero totale di moltiplicazioni per la risoluzione di un sistema lineare con il metodo di Cramer è:
$$
n!(n-1)(n+1) \sim n!n^{2}
$$
ignorando gli $n$ di ordine minore al secondo.

```ad-Teo
title: Prop - Costo computazionale
Il [[#Costo computazionale per Cramer]] è:
$$
n!(n-1)(n+1) \sim n!n^{2}
$$
```


Un computer modesto impiega circa $10^{-2} \mathrm{ s}$ per una operazione *pesante*, che corrisponde a $1 \, \mathrm{Gflops}$ dove un *flops* è il numero di operazioni eseguibili in un secondo.

Vediamo che già per $n = 15$, il costo computazionale ammonta a circa 3 giorni!

## Metodo di eliminazione di Gauss
Consiste nel ridurre a scala la matrice dei coefficienti.
Anche il costo computazionale di questo metodo è molto elevato: per $n$ grandi, è dell'ordine di 

- [0] $C_{c} \sim n^{3}/3$.


# Metodi iterativi
```ad-Definizione
title: Metodi iterativi

Si tratta di metodi che sfruttano il **Teorema del punto unito**.

```

Sono soggetti a errori anche in assenza di errori di approssimazione, in quanto richiedono in teoria un numero infinito di iterazioni. Pertanto sono soggetti a [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Errore di troncamento\|errore di troncamento]].

Si adattano bene alla soluzione di [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Matrici sparse\|#Matrici sparse]], e di grandi dimensioni ($n> 1000, n>>1000$)

## Metodo delle approssimazioni successive
Consiste nel trasformare il sistema
$$
AX = B
$$
in una forma equivalente
$$
X = \Phi (X)
$$
con $\Phi : \mathbb{R}^{n} \to \mathbb{R}^{n}$ **funzione di iterazione**.

Sia $\overline X$ la soluzione esatta, questa risulterà essere [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/03. Equazioni non lineari#Equazione di punto unito\|punto unito]] di $\Phi$ in quanto: 
$$
\overline X = \Phi(\overline X)
$$

### ☑️ Ipotesi

```ad-tip
title: Ipotesi

Elencare le ipotesi del metodo

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗
```

### Algoritmo
Dato il sistema 
$$
AX = B
$$
lo scriviamo come
$$AX - B = 0$$
Equivalente a

$$
X = CX + Q
$$
dove $CX + Q = \Phi(X)$.
- [1] $C \in \mathbb{R}^{n \times n}=$ [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Matrice di iterazione\|#Matrice di iterazione]] 
- [1] $Q \in \mathbb{R}^{n}$

```ad-tip
title: Schema numerico

$$
\begin{cases}
X^{(0)} \in \mathbb{R}^{n} &\quad \text{Approssimazione iniziale} \\
X^{(k+1)} = \Phi(X^{(k)}) = CX^{(k)} + Q &\quad k = 1,2,...
\end{cases}
$$


```

#### Matrice di iterazione
(<[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Algoritmo\|#Algoritmo]])
```ad-Definizione
title: Matrice di Iterazione
La matrice $C$ in 
$$
X = CX+Q
$$
prende il nome di **matrice di iterazione**

```

### Errori
Valutazione degli errori

#### Errore di troncamento

```ad-Definizione
title: Errore di troncamento

[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Errore di troncamento\|Errore di troncamento]] è definito come:

$$E^{(k+1)} = \overline X - X^{(k+1)}$$
```

```ad-Teo
title: Errore di troncamento

L'**errore di troncamento** in un metodo iterativo è
$$
E^{(k+1)} = CE^{(k)} = \underbrace{ C^{(k+1)} }_{C\cdot C\cdot... \cdot C \text{ k+1 volte}}E^{(0)}
$$
___
**Dimostrazione:**

Dimostriamo un'uguaglianza alla volta. La prima:
$$
\begin{align}
E^{(k+1)} &= \overline X - X^{(k+1)} = \\

&= (C \overline X + Q) - (C X^{(k)} + Q) = \\

&= C \overline X - C  X^{(k+1)} = \\

&= C(\overline X - X^{(k+1)}) = \\

&= CE^{(k)}
\end{align}
$$
La seconda uguaglianza invece:
$$
\begin{align}
E^{(k+1)} &= CE^{(k)} = \\
&= C(C+E^{(k-1)}) = \\
&= C^{2}E^{(k-1)} = \\
&= C^{3}E^{(k-2)} = \\
& \vdots \\
&= C^{(k+1)}E^{(0)}
\end{align}
$$

*c.v.d.*

```

Da ciò deriva che
$$
\lim_{k\to \infty} \left \| E^{(k)} \right \|= 0 \iff \lim_{k\to \infty} \left\|  C^{k} E^{(0)} \right \|= 0
$$
In una qualsiasi [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/02. Richiami di algebra lineare#Norma\|norma]]. 

### Convergenza
```ad-Definizione
title: Convergenza


Il metodo converge se
$$
\lim_{k \to \infty} \left \| E^{(k)} \right \| = \lim_{k\to \infty} \left\|  C^{k} E^{(0)} \right \|= 0
$$
```



#### Condizioni di convergenza
##### Condizione Necessaria
```ad-Teo
title: Teorema

Condizione necessaria alla convergenza è che
$$
\lim_{k \to \infty} X^{(k)} = \overline X
$$

```


##### Condizione Sufficiente
```ad-Teo
title: CS di convergenza

**Condizione Sufficiente** affinché il metodo converga è che la [[#Matrice di iterazione]]

$$
||C|| < 1
$$
___

**Dimostrazione**

Combinando la [[02. Richiami di algebra lineare#Relazione di Compatibilità|relazione di compatibilità]] e la [[02. Richiami di algebra lineare#Norma di matrice|norma del prodotto]], scriviamo:

$$
\begin{align}
\left \| C^{k}E^{(o)} \right \| &= \left \| C \cdot C \cdot C \cdots  C \cdot E^{(o)} \right \|  
\le 
\left \|  C \cdots  C \right \| \cdot \left \|  E^{(o)} \right \| \le \\
&\le \left \|  C \right \| \cdot\left \|  C \right \| \cdots \left \|  C \right \| \cdot \left \|  E^{(o)} \right \| \le \\
&\le \left \|  C \right \|^{k} \left \|  E^{(o)} \right \|
\end{align}
$$
Un metodo converge se è verificato questo [[#Convergenza|limite]]:
$$
\lim_{k \to \infty} \left \| E^{(k)} \right \| = \lim_{k\to \infty} \left\|  C^{k} E^{(0)} \right \|= 
\left[ \lim_{k \to \infty} \left \| C \right \|^{k}\right] \left \| E^{(0)} \right \| = 
0
$$
che è verificato se e solo se $||C|| < 1$.

*c.v.d.*


```

##### Condizione Necessaria & Sufficiente
```ad-Teo
title: Teorema

Condizione Necessaria e Sufficiente alla Convergenza del metodo è che il [[02. Richiami di algebra lineare#Raggio Spettrale|raggio spettrale]] della [[#matrice di iterazione]] sia [[02. Richiami di algebra lineare#Convergenza|convergente]], il che avviene se è rispettata la relazione
$$
\rho(C) < 1
$$

```


#### Ordine di convergenza
[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Ordine di convergenza\|Ordine di Convergenza]]


### Criterio di arresto
Fornire dei criteri di arresto, se pertinente
#### Criterio di arresto a Posteriori
$$
\left \| E^{(k)} \right \| \stackrel{k \to \infty}{\approx} \left \| X^{(k)}- X^{(k-1)} \right \| \le \varepsilon
$$
#### Criterio di arresto a Priori
```ad-Teo
title: Teorema

Mi posso fermare dopo aver eseguito un numero di iteriazioni $k$, tale che

$$
k \ge \log{\left( \frac{\varepsilon}{\left \| X^{(1)} - X^{(0)} \right \|} \right)} \frac{1}{\log{||C||}}
$$
```

$$
\begin{align}
\left \| E^{(k)} \right \| &= \left \| \overline X- X^{(k)} \right \| \\
&= \left \| \underbrace{\overline X- X^{(k+1)} }_{E^{(k+1)}} + X^{(k+1)} + X^{(x)} \right \| \le \left \| E^{(k+1)} \right \| + \left \| X^{(k+1)} + X^{(x)} \right \|
\end{align}
$$

### Velocità Asintotica di Convergenza
```ad-Teo
title: Velocità Asintotica di Convergenza

$$
V = -\log_{10} (\rho(C))
$$

```

### Esempi

```ad-example
title: Esempio
Trovare per quali valori reali di $\beta$ lo schema seguente converge


$$
\begin{cases}
x^{(k+1)} = \beta y^{(k)} - \frac{\beta}{2}z^{(k)} + \frac{7}{8} \\
y^{(k+1)} = \beta y^{(k)} - \frac{\beta}{2}z^{(k)} + \frac{7}{8} \\
z^{(k+1)} = - \frac{1}{2}y^{(k)} + \frac{1}{4} z^{(k)} - \frac{1}{2}
\end{cases}
$$

Si ha quindi:
$$
C =
\begin{bmatrix}
0 & \beta & - \frac{\beta}{2} \\ 0 & \beta & - \frac{\beta}{2} \\ 0  & - \frac{1}{2} & \frac{1}{4}
\end{bmatrix} 
\qquad
Q =
\begin{bmatrix}
\frac{7}{8} \\ \frac{7}{8}  \\ - \frac{1}{2}
\end{bmatrix}
$$
Verifichiamo la [[#Condizione Sufficiente]] di [[#Convergenza]], sia in norma 1 che in norma infinito.

$$
||C||_{1} = \max \left(0,2|\beta| + \frac{1}{2}, |\beta| + \frac{1}{4} \right) \le 1
$$
Che si verifica se 
$$
\begin{cases}
\beta < \frac{1}{4} &\quad \beta > 0 \\
\beta > - \frac{1}{4} &\quad \beta < 0
\end{cases}
$$

Oppure 
$$
||C||_{\infty} = \max \left( \frac{3}{2}|\beta|, \frac{3}{2} |\beta|, \frac{3}{4} \right) \le 1
$$
Verificato se
$$
\begin{cases}
\forall \beta &\quad |\beta| \le \frac{1}{2} \\
|\beta| < \frac{2}{3} &\quad |\beta| > \frac{1}{2}
\end{cases}
$$

```

## Metodo di Jacobi

<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/universita/2-anno/2-semestre/analisi-numerica/appunti/04-1-metodo-di-jacobi/" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">

$<div class="markdown-embed-title">

# Metodo di Jacobi

</div>




# Metodo di Jacobi
___

Siano
$
A \in \mathbb{R}^{n \times n} \qquad B \in \mathbb{R}^{n} \qquad C \in \mathbb{R}^{n \times n} \qquad Q \in \mathbb{R}^{n}
$
Data un sistema
$
AX = B
$
riscrivibile nella forma 
$
X = CX + Q
$

Riscrivo $A$ come somma di tre matrici:
$
A = D + L + U
$
dove

$
D =
\begin{bmatrix}
a_{11}  & 0 & \cdots  & 0 \\ 
0 & a_{22} & \cdots & 0 \\ 
\vdots & \vdots  & \ddots & 0 \\ 
0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\qquad
L =
\begin{bmatrix}
0  & 0 & \cdots  & 0 \\ 
a_{21} & 0 & \cdots & 0 \\ 
\vdots & \vdots  & \ddots & \vdots \\ 
a_{n1} & a_{n2} & \cdots & 0
\end{bmatrix}
\qquad
U = 
\begin{bmatrix}
0 & a_{12} & \cdots  & a_{1n} \\ 
0 & 0 & \cdots & a_{2n} \\ 
\vdots & \vdots  & \ddots & \vdots \\ 
0 & 0 & \cdots & 0
\end{bmatrix}
$

Sostituendo questa scomposizione nel problema, troviamo
$
\begin{align}
AX = (D+L+U) X &= B \\
DX + (L+U)X &= B \\
DX &= -(L+U) X + B \\
D^{-1}DX &= -D^{-1}\left[ (L+U)X + B \right] \\
X &= -D^{-1}[(L+U)X + B] \\ \\
X &= \underbrace{-D^{-1}(L+U)}_{ \stackrel{\triangle}{=} C_{J}}X + \underbrace{D^{-1}B}_{\stackrel{\triangle}{=} Q_{J}}
\end{align}
$
Il metodo di Jacobi consiste quindi nel risolvere
$
X = C_{J}X + Q_{J}
$

Scritta in forma estesa, $C_{J}$ e $Q_{J}$ diventano:

$
C_{J} = 
\begin{bmatrix}
0 & - \frac{a_{12}}{a_{11}} & \cdots\  & - \frac{a_{1n}}{a_{11}} \\ 
-\frac{a_{21}}{a_{22}} & 0 & \cdots\  & - \frac{a_{2n}}{a_{22}}
\\ \vdots  & \vdots & \ddots & \vdots \\   
-\frac{a_{n1}}{a_{nn}} & - \frac{a_{n2}}{a_{nn}} & \cdots\  & 0 \\ 
\end{bmatrix}
\qquad
Q_{J} = 
\begin{bmatrix}
\frac{b_{1}}{a_{11}} & 0 & \cdots &  0 \\ 
0 & \frac{b_{2}}{a_{22}} & \cdots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & \frac{b_{n}}{a_{nn}}
\end{bmatrix}
$



## ☑️ Ipotesi

```ad-tip
title: Ipotesi

Elencare le ipotesi del metodo
❗❗❗❗❗❗❗❗❗❗❗
❗❗❗COMPLETARE❗❗❗
❗❗❗❗❗❗❗❗❗❗❗
```

## Algoritmo
Lo schema iterativo è costituito da
$
X^{(k+1)} = C_{J}X^{(k)} + Q_{J}
$
Scrivendolo in forma esplicita diventa:

$
\begin{bmatrix}
x_{1}^{(k+1)} \\ x_{2}^{(k+1)} \\ \vdots \\ x_{n}^{(k+1)}
\end{bmatrix}
=
\begin{bmatrix}
0 & - \frac{a_{12}}{a_{11}} & \cdots\  & - \frac{a_{1n}}{a_{11}} \\ 
-\frac{a_{21}}{a_{22}} & 0 & \cdots\  & - \frac{a_{2n}}{a_{22}}
\\ \vdots  & \vdots & \ddots & \vdots \\   
-\frac{a_{n1}}{a_{nn}} & - \frac{a_{n2}}{a_{nn}} & \cdots\  & 0 \\ 
\end{bmatrix}
\begin{bmatrix}
x_{1}^{(k)} \\ x_{2}^{(k)} \\ \vdots \\ x_{n}^{(k)}
\end{bmatrix}
+
\begin{bmatrix}
\frac{b_{1}}{a_{11}} & 0 & \cdots &  0 \\ 
0 & \frac{b_{2}}{a_{22}} & \cdots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & \frac{b_{n}}{a_{nn}}
\end{bmatrix}
$

Questo diventa
$
\begin{align}
x_{1}^{(k+1)} &= - \frac{a_{12}}{a_{11}}x_{2}^{(k)} - - \frac{a_{13}}{a_{11}}x_{3}^{(k)} - \cdots - \frac{a_{1n}}{a_{11}}x_{n}^{(k)} - \frac{b_{1}}{a_{11}} = \frac{1}{a_{11}} \left( \sum\limits_{j=1, \,j \ne 1}^{n} \left (a_{1j}x_{j}^{(k)} \right) + b_{1} \right)
\end{align}
$
Facendo anche per le altre righe, possiamo riassumere e compattare con la scrittura:
$
x_{i}^{(k+1)} = \frac{1}{a_{ij}} \left( \sum\limits_{j=1, \, j \ne i}^{n}\left( a_{ij}x_{j}^{(k)} \right) + b_{i} \right) \qquad 1\le i \le n
$


## Errori
Valutazione degli errori
❗❗❗❗❗❗❗❗❗❗❗
❗❗❗COMPLETARE❗❗❗
❗❗❗❗❗❗❗❗❗❗❗
### Errore di troncamento
[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Errore di troncamento\|Errore di troncamento]]
❗❗❗❗❗❗❗❗❗❗❗
❗❗❗COMPLETARE❗❗❗
❗❗❗❗❗❗❗❗❗❗❗
## Convergenza
La convergenza è garantita secondo le condizioni della convergenza per sistemi lineari [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Condizioni di convergenza\|condizioni della convergenza per sistemi lineari]]:
$
\begin{align}
&\text{CS:}  &||C_{J}|| &<1 \\
&\text{CN:} &\lim_{k\to \infty}X^{(k)} &= \overline X \\
&\text{CNS:}  &\rho(C_{J})&<1 \\
\end{align}
$

```ad-Teo
title: (CS) - Matrici diagonalmente dominanti

Sia $A$ una matrice [[02. Richiami di algebra lineare#Matrice Diagonalmente Dominante Righe|Diagonalmente Dominante per Righe]] o per [[02. Richiami di algebra lineare#Matrice Diagonalmente Dominante per Colonne|Colonne]], 

**allora**

il metodo di Jacobi Converge.

```

```ad-Teo
title: Teorema

Il metodo converge se $A$ è una [[02. Richiami di algebra lineare#Matrice definita positiva|matrice definita positiva]].

```


### Ordine di convergenza
[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Ordine di convergenza\|Ordine di Convergenza]]


### Stima iterazioni necessarie
Fornire, se possibile, un modo di stimare le iterazioni necessarie


## Criterio di arresto
Fornire dei criteri di arresto, se pertinente
### Criterio di arresto a posteriori

## Implementazione in Matlab
Copiare il codice di implementazione in Matlab




</div></div>


## Metodo di Gauss-Seidel


<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/universita/2-anno/2-semestre/analisi-numerica/appunti/04-2-metodo-di-gauss-seidel/" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">

$<div class="markdown-embed-title">

# 4.2 Metodo di Gauss-Seidel

</div>




# Metodo di Gauss-Seidel
___

Siano

$
A \in \mathbb{R}^{n \times n} \qquad B \in \mathbb{R}^{n} \qquad C \in \mathbb{R}^{n \times n} \qquad Q \in \mathbb{R}^{n}
$
Data un sistema

$
AX = B
$
riscrivibile nella forma 


$
X = CX + Q
$

Riscrivo $A$ come somma di tre matrici:

$
A = D + L + U
$

dove

$
D =
\begin{bmatrix}
a_{11}  & 0 & \cdots  & 0 \\ 
0 & a_{22} & \cdots & 0 \\ 
\vdots & \vdots  & \ddots & 0 \\ 
0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\qquad
L =
\begin{bmatrix}
0  & 0 & \cdots  & 0 \\ 
a_{21} & 0 & \cdots & 0 \\ 
\vdots & \vdots  & \ddots & \vdots \\ 
a_{n1} & a_{n2} & \cdots & 0
\end{bmatrix}
\qquad
U = 
\begin{bmatrix}
0 & a_{12} & \cdots  & a_{1n} \\ 
0 & 0 & \cdots & a_{2n} \\ 
\vdots & \vdots  & \ddots & \vdots \\ 
0 & 0 & \cdots & 0
\end{bmatrix}
$


Sostituendo questa scomposizione nel problema, troviamo

$
\begin{align}
AX = (D+L+U) X &= B \\
(D+L)X + UX &= B \\
(D+L)X &= B - UX \\ \\
X &= \underbrace{-(D + L)^{-1}U}_{ \stackrel{\triangle}{=} C_{GS}}X + \underbrace{(D + L)^{-1}B}_{\stackrel{\triangle}{=} Q_{GS}}
\end{align}
$

Il metodo di Gauss-Seidel consiste quindi nel risolvere

$
X = C_{GS}X + Q_{GS}
$

Scritta in forma estesa, $C_{GS}$ e $Q_{GS}$ diventano:



## ☑️ Ipotesi

```ad-tip
title: Ipotesi


Elencare le ipotesi del metodo

```

## Algoritmo
Lo schema iterativo è costituito da

$
X^{(k+1)} = C_{GS}X^{(k)} + Q_{GS}
$

Facendo anche per le altre righe, possiamo riassumere e compattare con la scrittura:

$
x_{i}^{(k+1)} = \frac{1}{a_{ij}} \left(\sum\limits_{j=1, \, j \ne i}^{i-1}\left( a_{ij}x_{j}^{(k+1)} \right) + \sum\limits_{j=i+1, \, j \ne i}^{n}\left( a_{ij}x_{j}^{(k)} \right) + b_{i} \right) \qquad 1\le i \le n; \quad k \ge 0
$



## Errori
Valutazione degli errori
### Errore di troncamento
[[01. Introduzione ai metodi numerici#Errore di troncamento|Errore di troncamento]]

## Convergenza
La convergenza è garantita secondo le condizioni della convergenza per sistemi lineari [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/04. Sistemi di equazioni lineari#Condizioni di convergenza\|condizioni della convergenza per sistemi lineari]]:

$
\begin{align}
&\text{CS:}  &||C_{GS}|| &<1 \\
&\text{CN:} &\lim_{k\to \infty}X^{(k)} &= \overline X \\
&\text{CNS:}  &\rho(C_{GS})&<1 \\
\end{align}
$


```ad-Teo
title: Matrici diagonalmente dominanti

Sia $A$ una matrice [[02. Richiami di algebra lineare#Matrice Diagonalmente Dominante Righe|Diagonalmente Dominante per Righe]] o per [[02. Richiami di algebra lineare#Matrice Diagonalmente Dominante per Colonne|Colonne]], 

**allora**

il metodo di Gasuss-Seidel converge.

```


```ad-Teo
title: Teorema

Il metodo converge se $A$ è una [[02. Richiami di algebra lineare#Matrice definita positiva]]

```



### Ordine di convergenza
[[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Ordine di convergenza\|Ordine di Convergenza]]


### Stima iterazioni necessarie
Fornire, se possibile, un modo di stimare le iterazioni necessarie


## Criterio di arresto
Fornire dei criteri di arresto, se pertinente
### Criterio di arresto a posteriori

## Implementazione in Matlab
Copiare il codice di implementazione in Matlab




</div></div>


# Condizionamento
Nella realtà, gli input di un problema sono il più delle volte perturbati. Questo per via degli [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/01. Introduzione ai metodi numerici#Errore di troncamento\|errori di troncamento]] e per il fatto che i dati derivano da misure sperimentali.
Ci chiediamo come cambia l'output di un problema quando è presente una perturbazione nell'input.

## Problema ben condizionato
```ad-Definizione
title: Problema ben condizionato
Un problema si dice **ben condizionato** se una perturbazione nell'input induce piccole variazioni nell'output.

```

INPUT: $A,B$ 
$AX+B$
OUTPUT: $X$

![Pasted image 20230630125302.png](/img/user/Universit%C3%A0/2%C2%B0%20anno/2%C2%B0%20Semestre/Analisi%20Numerica/Appunti/allegati/Pasted%20image%2020230630125302.png)
Immaginiamo di avere un problema in cui solo $B$ sia perturbata. Da
$$
AX = B
$$
Otteniamo
$$
\begin{align}
A(X + \delta X) &= B + \delta B  \\
AX + A \delta X &= B + \delta  B
\end{align}
$$
Si noti come si ha all'interno il sistema lineare di partenza non perturbato. Sappiamo che $AX = B$ allora li posso cancellare dall'equazione.
$$
A \delta X = \delta B
$$
A questo punto, se esiste $A^{-1}$ scriviamo $\delta X$ come:
$$
\delta X = A^{-1} \delta B
$$

Ci interessa misurare quanto è grande la perturbazione. Passiamo alle [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/02. Richiami di algebra lineare#Norma\|norme]]. Vale, per la [[Università/2° anno/2° Semestre/Analisi Numerica/Appunti/02. Richiami di algebra lineare#Relazione di Compatibilità\|Relazione di Compatibilità]] la seguente disuguaglianza:
$$
||\delta X|| = ||A^{-1} \delta B || \le ||A^{-1} || \cdot ||\delta B||
$$
Dal sistema $||AX||=||B|| \le ||A||\cdot||X||$, ottengo:
$$
\frac{1}{||X||} \le \frac{||A||}{||B||}
$$
### Errore relativo
```ad-Teo
title: Errore relativo
$$
\frac{||\delta X||}{||X||} \le ||A^{-1}||||A|| \frac{|| \delta B||}{||B||}
$$
Dove $||A^{-1}||||A||$ è detto [[#Coefficiente di Amplificazione]]
```

#### Numero di Condizionamento
```ad-Definizione
title: Numero di Condizionamento
$$
K(A) = ||A^{-1}||\cdot||A|| \ge 1
$$
Nell'[[#Errore relativo]] il numero di condizionamento si riferisce al termine $||A^{-1}||\cdot||A||$ e dice, al massimo, di quanto viene **amplificato** l'errore sul termine noto.

Rispetta la seguente proprietà:
$$
1 \le K(A) \le \infty
$$
```

Sia $||\delta A||\cdot||A\mid< 1$ si dimostra che vale
$$
\frac{||\delta X||}{||X||} \le \frac{K(A)}{1- K(A) \frac{||\delta A||}{||A||}} \left(\frac{|| \delta B||}{||B||} + \frac{|| \delta A||}{||A||} \right)
$$

```ad-Teo
title: Teorema
Se la matrice $A$ è [[02. Richiami di algebra lineare#Matrice definita positiva|definita positiva]] **allora**
$$
K_{2}(A) = \frac{\max\limits_{i}\lambda_{i}(A)}{\min\limits_{i}\lambda_{i}(A)}
$$
dove $\lambda_{i}$ sono gli autovalori di $A$.
```

#### Matrice di Hilbert

```ad-Definizione
title: Matrice di Hilbert

$$
H =
\begin{bmatrix}
1 & \frac{1}{2} & \frac{1}{3} & \cdots & \frac{1}{n} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \cdots & \frac{1}{n+1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\frac{1}{n} & \frac{1}{n+1} & \frac{1}{n+2} & \cdots & \frac{1}{2n-1}
\end{bmatrix} \in \mathbb{R}^{n \times n}
$$

```

è una matrice mal condizionata




❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗ --> Fattorizzazione di LU e altra merda varia
❗❗❗❗❗❗❗❗❗❗❗❗❗

