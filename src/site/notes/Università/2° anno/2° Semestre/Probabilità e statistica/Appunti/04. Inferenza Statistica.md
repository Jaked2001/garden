---
{"dg-publish":true,"permalink":"/universita/2-anno/2-semestre/probabilita-e-statistica/appunti/04-inferenza-statistica/"}
---


# Definizioni generali
## Vettori di Variabili Aleatorie

```ad-Definizione
title: Vettori di VA


$$
X = (X_{1}, X_{2}, \cdots, X_{n}) \in \mathbb{R}^{n}
$$
Dove considereremo le $X_{k}$ [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/02. Misura e Probabilità#Eventi Indipendenti\|indipendenti]] tra loro

```


## Osservazioni
```ad-Definizione
title: Osservazioni ($x$)

Sono realizzazioni di $X$:
$$
x = (x_{1}, x_{2}, \cdots, x_{n})
$$
fatte su un certo campione

```


## Densità

Siano $X, Y$ due [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/03. Variabili Aleatorie\|Variabili Aleatorie]] [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/02. Misura e Probabilità#Indipendenza di eventi\|indipendenti]] continue,
$$
\begin{align}
\mathcal{P}(X\le x, Y \le y) &= F_{(X, Y)}(x, y) \\
\mathcal{P}(X\le x) \cdot \mathcal{P}(Y \le y) &= F_{X}(x) \cdot F_{Y}(y) 
\end{align}
$$

Derivando otteniamo:
$$
\begin{align}
\frac{\partial^{2}}{\partial x \partial y} F_{(X,Y)}(x,y) = F'_{X}(x)F'_{Y}(y) = f_{X}(x) f_{Y}(y) = f_{(X,Y)}(x, y)
\end{align}
$$
Dove distinguiamo tra [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Densità congiunta\|#Densità congiunta]] e [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Densità marginali\|#Densità marginali]]

Posso anche dire che la funzione di ripartizione congiunta è data dall'integrale
$$
F_{(X,Y)}(x,y) = \mathcal{P}(X\le x, Y \le y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{(X,Y)}(s,z) \, ds dz
$$
Inoltre
$$
\int_{\mathbb{R}} f_{(X,Y)}(x,y)   \, dx = f_{Y}(y) \qquad \qquad \int_{\mathbb{R}} f_{(X,Y)}(x,y)   \, dy = f_{X}(x)
$$

```ad-example
title: Esempio

Calcolare la [[#Densità congiunta]] $f_{X}(x), \quad x \in \mathbb{R}$ dato il [[#Vettori di Variabili Aleatorie]] $X= (X_{1}, X_{2}, ..., X_{n})$ nel caso in cui $X_{k} \sim \mathrm{Exp}(\lambda), \quad \lambda > 0 \qquad \forall k$.

$$
\begin{align}
f_{X} &= \prod_{k=1}^{n} f_{X_{k}}(x_{k}) = \\
&= \prod_{k=1}^{n}\lambda e^{-\lambda x_{k}}1_{(0, \infty)}(x_{k}) =  \\
&= \lambda^{n} e^{-\lambda \sum\limits_{k=1}^{n}x_{k}} 1_{(0,\infty)}(x_{k})
\end{align}
$$

```

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗
altri esempi

### Densità congiunta
$$
f_{(X,Y)}(x,y)
$$
### Densità marginali
$$
f_{X}(x) \qquad f_{Y}(y)
$$

___
[[_Giornaliera/2023-05-04\|2023-05-04]]
___
# Inferenza Statistica

## Statistica descrittiva
```ad-Definizione
title: Statistica descrittiva

Parto da un campione o da alcune osservazioni
- $x = (x_{1},x_{2}, \cdots, x_n)$: Osservazioni in generale (variabile)
- $x_{OSS} =$ "$x$ osservato": La successione di numeri nota

```

## Media Campionaria
```ad-Definizione
title: Media Campionaria

$$
\overline x = \frac{1}{n} \sum\limits_{k=1}^{n} x_{k}
$$
La media dei valori che ho osservato

```

## Momento
### Momento campionario

```ad-Definizione
title: Momento campionario

$$
m_{r} = \frac{1}{n} \sum\limits_{k=1}^{n}(x_{k})^{r}
$$

```

$$
m_{1}= \overline x
$$


### Momento teorico

```ad-Definizione
title: Momento teorico

$$
M_{R} = EX^{r} = \begin{cases}
\int_\mathbb{R} x^{r}f_{X}(x)  \, dx  \\
\sum\limits_{k}(x_{k})^{r}p_{k}
\end{cases}
$$

```


## Famiglia di distribuzioni di probabilità
```ad-Definizione
title: Famiglia di distribuzione di probabilità

Date
- $f_{X}(x) = f_{X}(x,\theta)$ dove $\theta$ è il parametro che caratterizza $X$ [[03. Variabili Aleatorie#Variabili aleatorie continue|continua]]
- $p_{k} = p_{k}(\theta)$ dove $\theta$ è il parametro che caratterizza $X$ [[03. Variabili Aleatorie#Variabili aleatorie DISCRETE|discreta]]

Si dice famiglia di distribuzione:
- $\mathcal{F} \{ f_{X}(x, \theta), x \in \mathbb{R}: \theta \in \mathbb{R}^{m} \}$: $X$ continua
- $\mathcal{F} \{ p_{k}(\theta), k \in \mathbb{Z}: \theta \in \mathbb{R}^{m} \}$: $X$ discreta

```

```ad-example
title: Esempio


$$
\left\{ \frac{e^{\frac{(x-\mu)^{2}}{2}}}{\sqrt{2 \pi}}, x \in \mathbb{R}: \mu \in \mathbb{R}  \right\}
$$
la famiglia delle densità $N(\mu, \sigma^{2} = 1)$ con $\theta  = \begin{bmatrix} \mu \\ 1 \end{bmatrix}$, $\mu \in \mathbb{R}$.
___

$$
\left\{ e^{\lambda} \frac{\lambda^{k}}{k!}, k \in \mathbb{N_{0}}: \lambda > 0  \right\}
$$

la famiglia delle densità $\mathrm{Pois}(\lambda)$ con $\theta = \lambda \in (0, \infty)$ 

```

## Massima affidabilità

Massimizzare l'affidabilità del risultato (già ottenuto)

Suppongo che la popolazione $\mathcal{P}$ oggetto di studio si distribuisca in accordo con $X_{1}$ con densità $f_{X_{1}}$.

Tutte le osservazioni provengono dalla popolazione $\mathcal{P} \sim X_{1}$, quindi:
$$
X = (X_{1}, \cdots, X_{n})
$$
è tale che $X_{k} \sim X_{1} \quad \forall k$, inoltre le $X_{k}$ sono [[02. Misura e Probabilità#Eventi Indipendenti|indipendenti]]

In generale 
$$
\max_{\theta} \mathcal{P}(X \in I) = \max \int_{I} f_{X}(x, \theta ) \, dx
$$
Per massimizzare l'affidabilità, esistono 2 metodi:
1. [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Metodo della massima Verosimiglianza\|#Metodo della massima Verosimiglianza]]
2. [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Metodo dei Momenti\|#Metodo dei Momenti]]

#### Funzione di verosimiglianza
```ad-Definizione
title: Funzione di verosimiglianza

$$
L (\theta,x) = f_{X}(x, \theta)
$$
Dove $x$ è dato e $\theta$ è da "stimare". In questo caso $\theta$ è la variabile di interesse.
$L$ è continua rispetto alla variabile $\theta$.
Devo massimizzare. Cerco 
$\hat \theta_{MV}$, il punto di massimo per $L$.

```

### Metodo della massima Verosimiglianza

Consiste nel trovare i punti di massimo della [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Funzione di verosimiglianza\|#Funzione di verosimiglianza]] imponendo

```ad-Teo
title:
$$
\frac{dL}{d \theta}(\theta,x) \stackrel{!}{=} 0
$$

```

```ad-example
title: Esempio

Sia $\mathcal{P} \sim N(\mu, 1)$
$X = (X_{1}, \cdots, X_{n})$ dove $X_{k} \sim N(\mu, 1) \quad \forall k$

Dato il campione $x = (x_{1}, \cdots, x_{n})$, voglio determinare $\hat \mu_{MV}$, ossia la stima di $\mu$ ottenuta con il [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Metodo della massima Verosimiglianza\|#Metodo della massima Verosimiglianza]]

___

Scrivo la [[#Funzione di verosimiglianza]]
$$
\begin{align}
L(\mu, x) &= f_{X}(x, \mu) = \\
&= \prod_{k=1}^{n} f_{X_{k}} (x, \mu) = \\
&= \prod_{k=1}^{n} \frac{e^{-\frac{(x_{k}-\mu)^{2}}{2}}}{\sqrt{2 \pi} } = \\
&= \frac{1}{\sqrt{(2 \pi)^{n}}} e^{-\frac{1}{2} \sum\limits_{k=1}^{n} (x_{k}-\mu)^{2} } 
\end{align}
$$
Posso ora derivarla e imporre uguale a 0:

$$
\begin{align}
\frac{dL}{d \mu} &= \frac{d}{d\mu} \frac{1}{(2 \pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \sum\limits_{k=1}^{n} (x_{k}-\mu)^{2} } \\
&= \frac{1}{(2 \pi)^{\frac{n}{2}}} \frac{d}{d\mu} e^{-\frac{1}{2} \sum\limits_{k=1}^{n} (x_{k}-\mu)^{2} } = \\
&= \frac{1}{(2 \pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \sum\limits_{k=1}^{n} (x_{k}-\mu)^{2} } \left(  \frac{1}{ \cancel{2}} \cancel{2} \sum\limits_{k=1}^{n} (x_{k} - \mu)  \right) = \\
&= \frac{1}{(2 \pi)^{\frac{n}{2}}} \left( \sum\limits_{k=1}^{n} (x_{k} - \mu)   \right) e^{-\frac{1}{2} \sum\limits_{k=1}^{n} (x_{k}-\mu)^{2} }
\end{align}
$$

Impongo uguale a zero per trovare i massimi e ottengo:

$$
\hat \mu_{MV} = \frac{1}{n} \sum\limits_{k=1}^{n} x_{k} = \overline x
$$

Notiamo che lo stimatore di massima verosimiglianza è proprio la media campionaria. 
La media campionaria è quindi la migliore stima per la media vera.

```

```ad-example
title: Esempio

Sia $\mathcal{P} \sim \mathrm{Exp}(\lambda), \quad \lambda > 0$
$X = (X_{1}, \cdots, X_{n})$ dove $X_{k} \sim \mathrm{Exp}(\lambda)$

Dato il campione $x = (x_{1}, \cdots, x_{n})$, voglio determinare $\hat \lambda_{MV}$, ossia la stima di $\lambda$ ottenuta con il [[#Metodo della massima Verosimiglianza]]
___
Scrivo la [[#Funzione di verosimiglianza]]
$$
\begin{align}
L(\lambda, x) &= f_{X_{k}}(x, \lambda) = \\
&= \lambda^{n} e^{-\lambda \sum\limits_{k=1}^{n}x_{k}} 
\end{align}
$$
Per cui derivo e impongo uguale a 0.

$$
\begin{align}
\frac{d}{d \lambda} L = n \lambda^{n-1} e^{-\lambda \sum\limits_{k=1}^{n}x_{k}} + \lambda^{n} \left(  e^{-\lambda \sum\limits_{k=1}^{n}x_{k}} \sum\limits_{k=1}^{n}x_{k} \right) \stackrel{!}{=} 0
\end{align}
$$
Posso riscrivere $\sum\limits_{k=1}^{n}x_{k} = n \overline x$.
$$
\begin{align}
\frac{d}{d \lambda} L = n \lambda^{n-1} e^{-\lambda n \overline x} + \lambda^{n} \left(  e^{-\lambda n \overline x} n \overline x \right) &\stackrel{!}{=} 0  \\
e^{-\lambda n \overline x} \left(n \lambda^{n-1} - \lambda^{n} n \overline x \right) &\stackrel{!}{=} 0 \\
\iff n \lambda^{n-1} &=   \lambda^{n} n \overline x \\
\cancel{n} \frac{\lambda^{n-1}}{\lambda^{n}} &= \cancel{n} \overline x \\
\lambda^{-1} &= \overline x
\end{align}
$$

per cui la stima di $\lambda$, $\hat{\lambda}_{MV}$ è
$$
\hat{\lambda}_{MV} = \frac{1}{\overline x}
$$

```

```ad-Teo
title: Teorema

Data una funzione monotona $h$, il punto di massimo per la [[#Funzione di verosimiglianza]] $L(\theta, x)$ coincide con il punto di massimo di $h(L(\theta, x))$

In particolare questa affermazione è vera se $h = \log$.

![GraficoMaxVerosimiglianzaLog.png](/img/user/Universit%C3%A0/2%C2%B0%20anno/2%C2%B0%20Semestre/Probabilit%C3%A0%20e%20statistica/Appunti/allegati/GraficoMaxVerosimiglianzaLog.png)
```

Questo ci permette di semplificare di molto i calcoli

```ad-example
title: Esempio

Ricalcoliamo $\hat{\lambda}_{MV}$ applicando il teorema appena enunciato

$$
L(\lambda, x) = \lambda^{n} e^{- \lambda n \overline x}
$$
calcolo
$$
\begin{align}
\ln(L(\lambda,x)) &= \ln(\lambda^{n}) + \ln(e^{-\lambda n \overline x})  \\
&= n\ln(\lambda) - \lambda n \overline x
\end{align}
$$
Derivo e impongo uguale a zero

$$
\begin{align}
\frac{d}{d \lambda}\ln(L(\lambda,x)) = \frac{n}{\lambda} - n \overline x &\stackrel{!}{=} 0 \\
n \left( \frac{1}{\lambda} - \overline x \right) &= 0 \\
\iff \\
\frac{1}{\lambda} = \overline x
\end{align}
$$
Quindi lo stimatore di [[#Metodo della massima Verosimiglianza|massima verosimiglianza]] è
$$
\hat{\lambda}_{MV} = \frac{1}{\overline x}
$$

```

❗❗❗❗❗❗❗❗❗❗❗❗❗
❗❗❗ COMPLETARE ❗❗❗
❗❗❗❗❗❗❗❗❗❗❗❗❗

Altri esempi

### Metodo dei momenti

```ad-Teo
title: Teorema

Metodo di stima parametrica dei momenti, dei parametri $\theta \in \mathbb{R}^{n}$. Devo risolvere il sistema:
$$
M_{R} = m_{r} \quad r = 1,2,\dots ,N
$$

Dove:
- $M_{R}$: [[#Momento teorico]] di ordine $R$
- $m_{r}$: [[#Momento campionario]] di ordine $r$

```

```ad-example
title: Esempio

Data una popolazione distribuita come una normale fornire una stima del parametro $\mu$ con il [[Università/2° anno/2° Semestre/Probabilità e statistica/Appunti/04. Inferenza Statistica#Metodo dei momenti\|#Metodo dei momenti]]:
$\mathcal{P} \sim N(\mu, \sigma^{2} = 2)$
___
Il parametro è a una sola dimensione, quindi basta un'equazione.
$$
\begin{align}
M_{1} &= m_{1} \\
EX &= \overline x \\
\int_\mathbb{R} x f_{X}(x) \, dx  &= \overline x
\end{align}
$$

Essendo una normale, la media teorica è proprio $\mu$, per cui la stima è data dalla media campionaria:
$$
\hat{\mu}_{MOM} = \overline x
$$

```

## Variabile aleatoria Stimatore
#ArgomentoFondamentale 

Data una popolazione $P$ caratterizzata dal parametro $\theta$.

- $\theta$ è il valore vero, esatto, del parametro di $P$.

Abbiamo definito la stima di $\theta$ il valore $\hat{\theta}$, ricavabile dal [[#Metodo della massima Verosimiglianza]] o dal [[#Metodo dei momenti]].
$\hat{\theta} = g(x)$ è una funzione dell'osservazione $x = (x_{1}, x_{2}, \dots, x_{n})$: campione della popolazione, di numerosità $n \in \mathbb{N}$.
$$
n \to \infty \quad \Longrightarrow \quad \hat{\theta} \to \theta
$$

Si ha quindi che $\hat{\Theta}$ è la variabile aleatoria stimatore.
$\hat{\Theta} = g(X)$ dove $X = (X_{1}, X_{2}, ..., X_{n})$ è la variabile aleatoria di $x$ e $X_{k} \sim P \quad \forall k$.

### Proprietà desiderabili dello stimatore

```ad-Teo
title: Proprietà desiderabili dello stimatore

Le proprietà desiderabili della [[#Variabile aleatoria Stimatore]] sono la
- [[#Correttezza]]
- [[#Consistenza]]

```

#### Correttezza

```ad-Definizione
title: Correttezza
$$
E\hat{\Theta} = \theta
$$
```

#### Consistenza

```ad-Definizione
title: Consistenza
$$
\lim_{ n \to \infty } \mathrm{Var}(\hat{\Theta}) = 0 \iff \mathcal{P}(\hat{\Theta} = \theta) = 1
$$
Con $n =$ Numerosità campionaria
```
